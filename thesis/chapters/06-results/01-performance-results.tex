\section{Performance Results}

This section presents the ELO ratings and performance metrics across all experimental configurations, establishing the baseline for evaluating the effectiveness of selective layer aggregation strategies.

\subsection{Final ELO Ratings}

Table~\ref{tab:final-elo} presents the final ELO ratings achieved by each configuration at round 350, the conclusion of training. The no sharing baseline (B2) achieved the highest average ELO of 1150.0, with the tactical cluster reaching 1225 and the positional cluster 1075. This represents a 137.5 ELO improvement over the full sharing baseline (B1), which averaged 1012.5 ELO.

\begin{table}[h]
\centering
\caption{Final ELO ratings at round 350}
\label{tab:final-elo}
\begin{tabular}{lcccccc}
\hline
\textbf{Experiment} & \textbf{Tactical} & \textbf{Positional} & \textbf{Average} & \textbf{$\Delta$ vs B1} & \textbf{$\Delta$ vs B2} \\
\hline
B1 (Full) & 1025 & 1000 & \textbf{1012.5} & baseline & -137.5 \\
B2 (None) & 1225 & 1075 & \textbf{1150.0} & +137.5 & baseline \\
P1 (Early) & 1075 & 1125 & \textbf{1100.0} & +87.5 & -50.0 \\
P2 (Middle) & 1150 & 1125 & \textbf{1137.5} & +125.0 & -12.5 \\
P3 (Late) & 1050 & 1050 & \textbf{1050.0} & +37.5 & -100.0 \\
P4 (Backbone) & 925 & 950 & \textbf{937.5} & -75.0 & -212.5 \\
\hline
\end{tabular}
\end{table}

Among the partial sharing configurations, P2 (share middle layers) performed best with 1137.5 average ELO, only 12.5 points below B2. P1 (share early layers) achieved 1100.0 ELO, while P3 (share late layers plus heads) reached 1050.0 ELO. Surprisingly, P4 (share all backbone, keep heads independent) exhibited the worst performance at 937.5 ELO, falling 75 ELO below even the full sharing baseline. This contradicts the hypothesis that extensive sharing with independent heads would achieve optimal performance.

The performance ranking from highest to lowest is: B2 (1150) > P2 (1138) > P1 (1100) > P3 (1050) > B1 (1013) > P4 (938). These results suggest that either complete independence or moderate selective sharing (around 30\% of parameters) yields superior performance compared to extensive sharing.

\subsection{Training Progression}

While final ELO provides a snapshot of ultimate performance, average ELO across all training rounds reveals learning dynamics and stability. Table~\ref{tab:avg-elo} presents the mean ELO computed over all 200 training rounds.

\begin{table}[h]
\centering
\caption{Average ELO across all training rounds}
\label{tab:avg-elo}
\begin{tabular}{lccc}
\hline
\textbf{Experiment} & \textbf{Tactical Avg} & \textbf{Positional Avg} & \textbf{Overall Avg} \\
\hline
B1 (Full) & 894.9 & 897.1 & \textbf{896.0} \\
B2 (None) & 920.7 & 925.0 & \textbf{922.9} \\
P1 (Early) & 931.4 & 907.9 & \textbf{919.6} \\
P2 (Middle) & 910.0 & 942.1 & \textbf{926.1} \\
P3 (Late) & 908.6 & 919.3 & \textbf{913.9} \\
P4 (Backbone) & 904.3 & 912.9 & \textbf{908.6} \\
\hline
\end{tabular}
\end{table}

Interestingly, P2 achieves the highest average ELO at 926.1, surpassing even B2's 922.9. This suggests that middle-layer sharing provides better training stability and sample efficiency, even though B2 ultimately reaches a higher final ELO. P1 also performs strongly with 919.6 average ELO, while P4 shows consistently poor performance at 908.6.

The divergence between average and final performance metrics reveals an important trade-off. Configurations with knowledge sharing (P2, P1) may learn more efficiently during training, achieving higher average performance across rounds. However, complete independence (B2) allows models to optimize further in later rounds, achieving superior final performance despite a lower average. This suggests that partial sharing improves early and mid-training phases but may impose constraints that limit final optimization.

\subsection{Performance Analysis}

The performance results reveal several critical insights. First, P2 (share middle layers) emerges as the optimal partial sharing configuration, achieving 98.9\% of B2's final performance (1137.5 vs 1150.0 ELO) while maintaining strong behavioral separation. The middle-layer sharing strategy appears to strike an effective balance between knowledge transfer and specialization capacity.

Second, P4's dramatic underperformance challenges the hypothesis that extensive sharing with independent heads would be optimal. Despite sharing 86\% of parameters and maintaining independent policy/value heads, P4 achieves the worst performance across all configurations. This suggests that the shared backbone creates optimization conflicts between the tactical and positional training objectives, with gradients from tactical opening games (Sicilian, King's Gambit) pushing toward sharp tactical patterns while gradients from positional opening games (Queen's Gambit, Nimzo-Indian) favor strategic calculation. The result appears to be a backbone stuck in a suboptimal compromise, with the 14\% of parameters in the heads insufficient to compensate.

Third, the consistent superiority of B2 over all partial sharing configurations indicates that federated knowledge transfer provides limited benefits in this chess reinforcement learning domain. Unlike image classification or language modeling where shared low-level features generalize across tasks, chess playstyle optimization appears highly specialized. Games from tactical openings versus positional openings may be sufficiently different that cross-cluster learning interferes more than it helps.

Fourth, all partial sharing configurations outperform the full sharing baseline (B1) except P4, validating that some degree of specialization improves performance. However, the margin varies dramatically. P2 exceeds B1 by 125 ELO while P4 falls 75 ELO short. This demonstrates that layer selection for sharing versus specialization is the critical design choice, more important than whether to use selective sharing at all.

The performance ranking exhibits a clear pattern: configurations with moderate sharing (P2 at 32\%, P1 at 23\%) outperform both extreme sharing (B1 at 100\%, P4 at 86\%) and configurations that share decision-making layers (P3 with shared heads). This suggests an optimal sharing percentage exists in the range of 20-35\% of parameters, concentrated in middle layers rather than input or output layers.
