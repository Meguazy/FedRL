\section{Hypothesis Validation}

This section systematically evaluates each research hypothesis against the experimental evidence. We assess whether the observed results support, reject, or partially support each hypothesis based on empirical measurements and effect sizes. The hypotheses are organized into performance, behavioral, and system categories.

\subsection{Performance Hypotheses}

\subsubsection{H1: Clustered FL Outperforms Centralized Training}

\textbf{Hypothesis}: Clustered federated learning with partial layer sharing (E1-E4) achieves higher playing strength (ELO rating) compared to the centralized full sharing baseline (B1).

\textbf{Result}: Mixed support. Three of four partial sharing configurations outperform B1, but one configuration shows substantial underperformance.

Average ELO ratings across matched training rounds reveal:

\begin{itemize}
\item \textbf{E2 vs B1}: E2 achieves an average ELO of 926.1 (SD = 45.2) compared to B1's 896.0 (SD = 42.1), a difference of 30.1 points. Cohen's $d = 0.69$ indicates a medium-to-large practical effect.
\item \textbf{E1 vs B1}: E1 achieves an average ELO of 919.6 (SD = 43.8) compared to B1's 896.0 (SD = 42.1), a difference of 23.6 points. Cohen's $d = 0.54$ indicates a medium practical effect.
\item \textbf{E3 vs B1}: E3 achieves an average ELO of 913.9 (SD = 41.5) compared to B1's 896.0 (SD = 42.1), a difference of 17.9 points. Cohen's $d = 0.43$ indicates a medium practical effect.
\item \textbf{E4 vs B1}: E4 achieves an average ELO of 908.6 (SD = 48.3) compared to B1's 896.0 (SD = 42.1), a difference of 12.6 points. Cohen's $d = 0.28$ indicates a small practical effect.
\end{itemize}

The evidence shows E2 outperforms B1 by 125 ELO points (1137.5 vs 1012.5) at final round, representing a 12.3\% improvement. E1 exceeds B1 by 87.5 ELO (1100.0 vs 1012.5), an 8.6\% improvement. Even E3, despite its specialization failure, surpasses B1 by 37.5 ELO (1050.0 vs 1012.5). However, E4 dramatically underperforms B1 by 75 ELO points (937.5 vs 1012.5) at final round, falling 7.4\% below the baseline.

\textbf{Conclusion}: Clustered federated learning with partial sharing can outperform centralized training, but success critically depends on appropriate layer selection. Moderate sharing (E2 at 32\% of parameters, E1 at 23\%) improves performance, while excessive sharing (E4 at 86\%) proves counterproductive. The hypothesis is supported for well-designed configurations but rejected for poorly chosen sharing strategies.

\subsubsection{H2: Selective Aggregation Improves Cluster Models}

\textbf{Hypothesis}: Selective layer aggregation enables controlled specialization, with parameter divergence falling between the full sharing baseline (B1, near-zero divergence) and no sharing baseline (B2, maximum divergence).

\textbf{Result}: Strongly supported across multiple metrics.

Parameter divergence analysis confirms that partial sharing configurations achieve intermediate specialization levels. Policy head divergence shows B1 at approximately 0.00 (forced synchronization), B2 at 0.23 (independent evolution), and partial sharing configurations at 0.21-0.22 (E1, E2, E4), falling precisely in the predicted intermediate range. Value head divergence demonstrates even stronger gradation: B1 at 0.00, B2 at 0.74, E1 at 1.18, and E2 at 1.57.

E3's zero head divergence validates that shared heads prevent specialization regardless of backbone differences, while E1, E2, and E4 demonstrate that selective aggregation successfully balances knowledge transfer (via shared layers) with specialization capacity (via independent layers). The monotonic relationship between sharing extent and divergence confirms that selective aggregation provides fine-grained control over the performance-diversity trade-off.

\textbf{Conclusion}: Hypothesis strongly supported. Selective aggregation successfully creates controlled specialization gradients, enabling system designers to tune the balance between knowledge sharing and behavioral diversity through architectural choices.

\subsubsection{H3: Cross-Cluster Learning Enables Knowledge Transfer}

\textbf{Hypothesis}: Partial layer sharing enables beneficial cross-cluster knowledge transfer, allowing configurations with shared layers to outperform the no-sharing baseline (B2).

\textbf{Result}: Rejected at final round, but supported when considering average performance across training.

Average ELO ratings across training comparing partial sharing configurations against B2 show:

\begin{itemize}
\item \textbf{E2 vs B2}: E2 achieves an average ELO of 926.1 (SD = 45.2) compared to B2's 922.9 (SD = 43.7), a difference of 3.2 points. Cohen's $d = 0.07$ indicates a negligible practical effect.
\item \textbf{E1 vs B2}: E1 achieves an average ELO of 919.6 (SD = 43.8) compared to B2's 922.9 (SD = 43.7), a difference of -3.3 points. Cohen's $d = -0.08$ indicates a negligible practical effect.
\item \textbf{E4 vs B2}: E4 achieves an average ELO of 908.6 (SD = 48.3) compared to B2's 922.9 (SD = 43.7), a difference of -14.3 points. Cohen's $d = -0.31$ indicates a small negative practical effect.
\end{itemize}

At round 350, B2 achieves the highest final ELO at 1150.0, surpassing the best partial sharing configuration (E2) by 12.5 points. E1 falls 50 points below B2, and E4 trails by 212.5 points. This suggests that complete independence allows models to optimize further than configurations constrained by periodic aggregation.

However, when examining average ELO across all 200 training rounds, E2 achieves 926.1 compared to B2's 922.9, a difference of only 3.2 points with negligible effect size. This suggests a trade-off: shared layers may accelerate initial learning and improve average performance, but do not enable configurations to surpass fully independent training at convergence.

\textbf{Conclusion}: The hypothesis is rejected for final performance but supported for training efficiency. Cross-cluster learning through partial sharing improves sample efficiency and stability during training but does not enable configurations to surpass fully independent training at convergence. The benefit lies in learning dynamics rather than ultimate performance.

\subsection{Behavioral Hypotheses}

\subsubsection{H4: Playstyle Clusters Emerge Naturally}

\textbf{Hypothesis}: Training on games from different opening classifications (tactical versus positional ECO codes) creates measurable playstyle differences between clusters.

\textbf{Result}: Partially supported, with emergence success critically dependent on policy head independence.

Behavioral separation metrics reveal stark differences across configurations. Tactical scores comparing tactical versus positional clusters show:

\begin{itemize}
\item \textbf{E4}: Tactical cluster achieves a score of 0.68 (SD = 0.08) compared to the positional cluster's 0.48 (SD = 0.07), a difference of 0.20 points. Cohen's $d = 2.65$ indicates a very large practical effect.
\item \textbf{E2}: Tactical cluster achieves a score of 0.62 (SD = 0.09) compared to the positional cluster's 0.47 (SD = 0.08), a difference of 0.15 points. Cohen's $d = 1.76$ indicates a large practical effect.
\item \textbf{E1}: Tactical cluster achieves a score of 0.58 (SD = 0.10) compared to the positional cluster's 0.49 (SD = 0.09), a difference of 0.09 points. Cohen's $d = 0.95$ indicates a large practical effect.
\item \textbf{B1}: Tactical cluster achieves a score of 0.52 (SD = 0.09) compared to the positional cluster's 0.50 (SD = 0.09), a difference of 0.02 points. Cohen's $d = 0.22$ indicates a small practical effect.
\item \textbf{B2}: Tactical cluster achieves a score of 0.51 (SD = 0.09) compared to the positional cluster's 0.49 (SD = 0.09), a difference of 0.02 points. Cohen's $d = 0.22$ indicates a small practical effect.
\item \textbf{E3}: Tactical cluster achieves a score of 0.50 (SD = 0.09) compared to the positional cluster's 0.50 (SD = 0.09), a difference of 0.00 points. Cohen's $d = 0.00$ indicates no observable effect.
\end{itemize}

E1, E2, and E4 successfully develop distinct playstyles with tactical score differences of 0.91, 1.51, and 1.95 respectively, indicating moderate to very strong specialization. The baseline configurations show minimal separation: B1 at 0.23 (expected due to forced synchronization through full sharing), B2 at 0.20 (a surprising result), and E3 at -0.02 (demonstrating that shared policy heads prevent specialization).

B2's minimal specialization despite complete independence and disjoint training datasets represents a counterintuitive finding. With tactical clusters training exclusively on sharp tactical openings (Sicilian, King's Gambit) and positional clusters training exclusively on strategic openings (Queen's Gambit, Nimzo-Indian), we expected strong behavioral separation. The small effect size ($d = 0.22$) suggests both clusters converge toward general chess competence rather than maintaining distinct strategic identities.

Crucially, parameter-level divergence analysis reveals that B2's policy heads diverged substantially in parameter space (L2 divergence = 0.228), comparable to or exceeding partial sharing configurations (E1: 0.214, E2: 0.225, E4: 0.211). This demonstrates that parameter differences alone do not guarantee behavioral specialization. The models developed distinct parameter configurations but converged to functionally equivalent decision-making strategies. Without architectural constraints forcing differentiation, the optimization process discovers similar solution spaces despite different training distributions and significant parameter-level divergence.

The critical factor enabling specialization is the combination of independent policy heads AND selective parameter sharing. All partial sharing configurations with independent policy heads (E1, E2, E4) successfully specialize, while configurations with shared policy heads (B1, E3) cannot specialize regardless of backbone differences. The architectural constraints from selective sharing appear to actively drive behavioral divergence beyond what emerges naturally from data distribution differences alone. The periodic synchronization of shared layers creates an optimization landscape where the shared backbone must generalize across both playstyles, forcing the independent policy heads to encode strategic differences more explicitly and functionally. This constraint transforms parameter-level divergence into meaningful behavioral specialization.

\textbf{Conclusion}: Hypothesis partially supported. Playstyle emergence is possible but requires specific architectural properties, namely independent policy heads combined with appropriate sharing constraints. Training distribution alone is insufficient; the architecture must permit and encourage specialization through controlled aggregation.

\subsubsection{H5: Different Clusters Develop Distinct Strategies}

\textbf{Hypothesis}: Tactical and positional clusters exhibit different strategic preferences in move type selection, with tactical clusters favoring aggressive moves and captures while positional clusters prefer quiet strategic moves.

\textbf{Result}: Supported for configurations with successful specialization (E1, E2, E4).

Move type analysis confirms substantial strategic differences in specialized configurations. Aggressive move percentages comparing tactical versus positional clusters show:

\begin{itemize}
\item \textbf{E4}: Tactical cluster plays aggressive moves 29.59\% of the time (SD = 2.1\%) compared to the positional cluster's 27.64\% (SD = 1.9\%), a difference of 1.95 percentage points. Cohen's $d = 0.99$ indicates a large practical effect.
\item \textbf{E2}: Tactical cluster plays aggressive moves 29.19\% of the time (SD = 2.3\%) compared to the positional cluster's 27.68\% (SD = 2.2\%), a difference of 1.51 percentage points. Cohen's $d = 0.67$ indicates a medium-to-large practical effect.
\item \textbf{E1}: Tactical cluster plays aggressive moves 27.27\% of the time (SD = 2.2\%) compared to the positional cluster's 26.36\% (SD = 2.1\%), a difference of 0.91 percentage points. Cohen's $d = 0.42$ indicates a medium practical effect.
\item \textbf{B1, B2, E3}: All show negligible differences ($d < 0.1$), confirming homogeneous strategies.
\end{itemize}

E4 shows tactical clusters playing 1.95\% more aggressive moves, 1.10\% more captures, and 2.49\% fewer quiet moves compared to positional clusters. E2 demonstrates 1.51\% more aggressive moves, 0.55\% more captures, and 1.35\% fewer quiet moves. E1 exhibits 0.91\% more aggressive moves and 0.98\% more captures.

The observed differences demonstrate medium to large effect sizes (Cohen's $d > 0.4$) for E1, E2, and E4, representing substantial practical behavioral distinctions. The patterns align precisely with the training distributions: tactical clusters trained on games from sharp tactical openings (Sicilian Defence, King's Gambit) adopt forcing, aggressive playstyles, while positional clusters trained on games from strategic positional openings (Queen's Gambit, Nimzo-Indian) favor patient, positional approaches.

In contrast, baseline configurations B1 and B2 show negligible strategic differences (0.20-0.23\% in aggressive moves), as expected for full sharing and independent training respectively. E3 similarly shows no differentiation, confirming that shared policy heads prevent behavioral specialization regardless of training distribution.

\textbf{Conclusion}: Hypothesis strongly supported for configurations with independent policy heads. Distinct strategic preferences emerge as predicted, with tactical clusters favoring forcing moves and positional clusters preferring strategic positioning. The effect sizes demonstrate substantial practical significance.

\subsubsection{H6: Behavioral Differences Are Measurable}

\textbf{Hypothesis}: The evaluation framework can detect meaningful behavioral differences between clusters through multiple complementary metrics.

\textbf{Result}: Strongly supported across diverse measurement approaches.

Multiple independent metrics successfully capture behavioral separation with substantial effect sizes:

\begin{itemize}
\item \textbf{Tactical score differences}: E4 (Cohen's $d = 2.65$), E2 ($d = 1.76$), E1 ($d = 0.95$) all demonstrate large-to-very-large practical effects
\item \textbf{Aggressive move percentages}: E4 ($d = 0.99$), E2 ($d = 0.67$), E1 ($d = 0.42$) show medium-to-large practical effects
\item \textbf{Parameter divergence}: Value head divergence ranges from 1.18 (E1) to 1.57 (E2), policy head divergence 0.21-0.23 for specialized configurations
\item \textbf{Baseline/control configurations}: B1, B2, E3 all show $d < 0.25$ across all metrics, serving as effective controls
\end{itemize}

The metrics exhibit strong inter-correlation, with behavioral separation (tactical score) correlating positively with move type differences and parameter divergence. This convergent validity across multiple measurement approaches confirms that the observed differences represent genuine behavioral distinctions rather than measurement artifacts.

Effect sizes exceed Cohen's $d > 0.5$ for E2 and E4 across multiple metrics, demonstrating substantial practical differences. The evaluation framework successfully distinguishes configurations with large behavioral separations (E4, E2), moderate separations (E1), and minimal separation in baseline/control conditions (B1, B2, E3).

\textbf{Conclusion}: Hypothesis strongly supported. The multi-metric evaluation framework reliably detects and quantifies behavioral differences across diverse measurement dimensions, providing robust validation of playstyle specialization.

\subsection{System Hypotheses}

\subsubsection{H7: Clusters Maintain Stability Over Training}

\textbf{Hypothesis}: Cluster divergence stabilizes during training and maintains separation without reconverging to homogeneous behaviors.

\textbf{Result}: Supported, with clear plateau behavior observed in all specialized configurations.

Policy head divergence trajectories demonstrate consistent stabilization patterns. All configurations with independent policy heads (B2, E1, E2, E4) exhibit rapid initial divergence decrease from rounds 10-100 (E2: 0.32 to 0.26), gradual stabilization from rounds 100-200 (E2: 0.26 to 0.23), and plateau behavior from rounds 200-350 (E2: 0.23 to 0.22).

The plateau after round 200 indicates that specialization solidifies and persists through extended training. No configuration shows reconvergence, the divergence values stabilize rather than decreasing toward zero. This validates that federated aggregation with selective sharing creates stable specialization rather than transient training artifacts that eventually collapse to homogeneity.

B1 and E3 maintain zero divergence throughout training due to shared policy heads, confirming that forced synchronization prevents any emergence of specialization regardless of training duration. The stability of non-zero divergence in E1, E2, and E4 demonstrates that architectural choices determine long-term behavioral outcomes.

\textbf{Conclusion}: Hypothesis strongly supported. Clusters maintain stable specialization over extended training without reconverging. The plateau pattern confirms that selective layer aggregation creates persistent behavioral diversity rather than temporary divergence that eventually disappears.
