\section{Evaluation Protocol}

This section describes when and how evaluations are conducted throughout the experimental campaign, including real-time metrics during training and post-hoc analysis after experiments complete.

\subsection{During-Training Evaluation}

Real-time evaluation occurs automatically during training at the frequencies specified in Section 5.5. The \texttt{ExperimentTracker} (Chapter 4.8) coordinates evaluation by:

\begin{enumerate}
\item Triggering evaluation at configured intervals (every round for core metrics, every 10 rounds for expensive metrics)
\item Saving current model checkpoints for each cluster
\item Calling evaluation modules (playstyle calculator, divergence calculator, move analyzer, ELO estimator)
\item Persisting results to the metrics store in JSON format
\item Resuming training for the next round
\end{enumerate}

Metrics are stored in \texttt{storage/metrics/\{run\_id\}/} with separate subdirectories for each cluster and metric type. This enables real-time monitoring of training progress and early detection of issues like divergence collapse or playstyle convergence.

\subsection{Post-Training Analysis}

After all 200 training rounds complete, post-hoc analysis is performed on the saved metrics and final models:

\textbf{Trajectory Analysis:}
Time-series plots visualize metric evolution across training rounds. Key trajectories include:
\begin{itemize}
\item ELO progression: Validates learning and convergence
\item Tactical score evolution: Tracks playstyle development
\item Divergence by layer group: Reveals when specialization occurs
\item Move type percentages: Shows behavioral trend shifts
\end{itemize}

\textbf{Plateau Detection:}
A rolling window algorithm (window size = 20 rounds) detects when metrics stabilize. A plateau is identified when the standard deviation within the window falls below a threshold (0.01 for normalized metrics). This determines convergence rounds for hypothesis H7.

\textbf{Correlation Analysis:}
Pairwise correlations between metrics reveal relationships:
\begin{itemize}
\item Tactical score vs. aggressive move percentage (expected: positive correlation)
\item Policy head divergence vs. playstyle separation (expected: positive correlation)
\item ELO vs. training round (expected: positive early, plateau later)
\item Divergence vs. training round (expected: increase then stabilize)
\end{itemize}

\textbf{Cross-Experiment Comparison:}
Final-round metrics from all six experiments (B1, B2, P1-P4) are aggregated into comparison tables and visualizations. This enables identification of the optimal configuration and validation of hypotheses H1, H2, and H5.

\subsection{Generalization Testing}

Generalization is assessed using held-out puzzle sets not seen during training. This validation occurs after all experiments complete:

\begin{enumerate}
\item Prepare benchmark sets: 100 tactical puzzles, 100 positional puzzles, 100 mixed puzzles
\item Load final models from each experiment
\item Evaluate each model on all three benchmark sets
\item Compute accuracy: percentage of puzzles where the model's top move matches the solution
\item Cross-test: Evaluate tactical models on positional puzzles and vice versa
\end{enumerate}

Success criterion for H9: Cross-domain accuracy $> 60\%$ (e.g., tactical model achieves $> 60\%$ on positional puzzles).

All analysis procedures are automated through Python scripts that load metrics from the structured JSON storage, perform statistical tests, and generate visualizations for inclusion in the results chapter.
