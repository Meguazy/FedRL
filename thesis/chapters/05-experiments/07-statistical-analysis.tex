\section{Statistical Analysis}

Statistical validation ensures that observed differences between experiments are meaningful rather than artifacts of random variation. This section defines the hypothesis tests, effect size metrics, and reporting standards applied to validate the seven research hypotheses.

\subsection{Hypothesis Testing Procedures}

Different comparisons require different statistical tests based on the data structure and hypotheses. For within-configuration comparisons, we use paired t-tests to compare partial sharing configurations (P1-P4) against baselines (B1, B2) using matched pairs from the same training rounds. The null hypothesis states no difference in ELO between configurations, while the alternative proposes that partial sharing achieves higher ELO than the baseline. This validates H1 (comparing against B1) and H3 (comparing against B2).

For cluster comparisons, independent samples t-tests compare tactical cluster versus positional cluster playstyle metrics. The null hypothesis states no difference in tactical scores between clusters, while the alternative allows for any significant difference. These tests validate H4 (playstyle emergence) and H5 (distinct strategies).

Multi-configuration comparison uses one-way ANOVA with Tukey HSD post-hoc tests to simultaneously compare all P1-P4 configurations. The null hypothesis states no difference in ELO across the four partial sharing experiments, while the alternative suggests at least one configuration differs significantly. The Tukey HSD post-hoc test identifies which specific pairs differ, helping identify the optimal layer sharing strategy.

When testing multiple hypotheses on the same dataset, we apply Bonferroni correction to control family-wise error rate. The corrected significance level is $\alpha_{\text{corrected}} = 0.05 / n_{\text{tests}}$. For example, comparing four partial sharing configurations requires six pairwise comparisons, yielding $\alpha = 0.05/6 \approx 0.008$.

\subsection{Effect Size Metrics}

Statistical significance only indicates an effect exists; effect size quantifies its practical importance. For t-tests, we compute Cohen's d as:
\begin{equation}
d = \frac{\mu_1 - \mu_2}{\sigma_{\text{pooled}}}, \quad \sigma_{\text{pooled}} = \sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}}
\end{equation}

where $\mu_1$ and $\mu_2$ are the means of the two groups being compared, $\sigma_1$ and $\sigma_2$ are the standard deviations of the two groups, and $\sigma_{\text{pooled}}$ is the pooled standard deviation representing the average variability across both groups.

Interpretation follows standard conventions: $|d| = 0.2$ represents a small effect, $|d| = 0.5$ a medium effect, and $|d| = 0.8$ a large effect. We apply Cohen's d to cluster playstyle differences and baseline comparisons.

For ANOVA, we calculate eta-squared ($\eta^2$) to represent the proportion of variance explained:
\begin{equation}
\eta^2 = \frac{SS_{\text{between}}}{SS_{\text{total}}}
\end{equation}

where $SS_{\text{between}}$ is the sum of squares between groups (variance explained by group differences) and $SS_{\text{total}}$ is the total sum of squares (total variance in the data). The ratio indicates what proportion of the total variance is attributable to differences between experimental configurations.

Interpretation: $\eta^2 = 0.01$ indicates a small effect, $\eta^2 = 0.06$ a medium effect, and $\eta^2 = 0.14$ a large effect. This metric applies to P1-P4 configuration comparisons.

All effect sizes include 95\% confidence intervals. If the confidence interval includes zero, the effect may not be reliable despite achieving statistical significance.

\subsection{Hypothesis Validation Criteria}

Each hypothesis has specific success criteria:

\begin{itemize}
\item \textbf{H1}: Best partial sharing ELO $>$ B1 ELO, paired t-test $p < 0.05$, Cohen's $d > 0.5$
\item \textbf{H2}: $0 < \text{divergence}(P_i) < \text{divergence}(B2)$ for all partial sharing configs
\item \textbf{H3}: Partial sharing ELO $>$ B2 ELO, paired t-test $p < 0.05$
\item \textbf{H4}: Cluster tactical scores significantly different, independent t-test $p < 0.001$, Cohen's $d > 0.8$
\item \textbf{H5}: Move type distributions differ, independent t-test on aggressive move \%, $p < 0.05$, $d > 0.5$
\item \textbf{H6}: Effect sizes for behavioral metrics $> 0.5$ (Cohen's d for move type comparisons)
\item \textbf{H7}: Plateau detected (divergence stabilizes) and no reconvergence in final 50 rounds
\end{itemize}

\subsection{Reporting Standards}

For transparency and reproducibility, we report comprehensive statistics for each test following APA guidelines. Descriptive statistics include mean ($M$), standard deviation ($SD$), and sample size ($n$) for each group. Test statistics specify the $t$-value with degrees of freedom for t-tests, where the notation $t(df)$ indicates the t-statistic value with $df$ degrees of freedom. For independent samples t-tests, degrees of freedom are calculated as $df = n_1 + n_2 - 2$, while paired t-tests use $df = n - 1$ where $n$ is the number of matched pairs. ANOVA results report $F$-statistics with $df_{\text{between}}$ and $df_{\text{within}}$.

Exact $p$-values are reported rather than inequality statements like "$p < 0.05$". Effect sizes include Cohen's $d$ with interpretation labels (small, medium, or large) based on conventional thresholds. Confidence intervals are provided at 95\% level for both mean differences and effect sizes. Each result concludes with a clear statement linking findings to the relevant hypothesis.

For example: "Tactical cluster ($M = 0.68$, $SD = 0.12$, $n = 350$) showed significantly higher tactical scores than positional cluster ($M = 0.45$, $SD = 0.11$, $n = 350$), $t(698) = 28.5$, $p < 0.001$, Cohen's $d = 1.92$, 95\% CI [0.21, 0.25]. This very large effect supports H4." Here, $t(698)$ means the t-statistic is 28.5 with 698 degrees of freedom (350 + 350 - 2).

All statistical analyses are performed using Python (scipy.stats, statsmodels) and results are included in the results chapter with supporting visualizations.
