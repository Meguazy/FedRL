\section{Results Analysis and Evaluation Methodology}

This section describes how experimental results are analyzed and compared to validate the seven research hypotheses. Rather than relying on formal statistical inference, we employ empirical comparison of performance metrics, effect size analysis, and convergence trajectory examination to assess hypothesis validity.

\subsection{Comparison Groups and Data Structure}

The analysis involves two types of comparisons, each with different group definitions and sample sizes.

For performance comparisons between configurations, each group represents one experimental configuration trained through 350 rounds. When comparing two configurations (for example, E2 versus B1), each group consists of 350 ELO measurements, one per training round. The comparison examines whether the two configurations achieve different performance levels across their training trajectories. Metrics include both final ELO (round 350) and average ELO (mean across all 350 rounds).

For behavioral comparisons within a configuration, each group represents one cluster (tactical or positional) evaluated across training rounds. When comparing tactical versus positional clusters within a configuration, each group again consists of 350 measurements of behavioral metrics (tactical scores, aggressive move percentages) collected at each training round. The comparison examines whether the two clusters exhibit different playing styles throughout training.

Effect sizes and descriptive statistics are computed from these time series measurements. While sequential training rounds exhibit temporal dependencies (each round builds on previous learning), the 350-round trajectories provide sufficient observations to characterize each configuration's performance and each cluster's behavioral patterns.

\subsection{Performance Comparison Approach}

Configuration performance is assessed through direct comparison of ELO ratings measured throughout training. Two primary metrics characterize each configuration: the final ELO rating achieved at round 350, representing ultimate playing strength after complete training, and the average ELO across all training rounds, indicating overall training efficiency and stability.

Performance differences are reported as both absolute ELO point differences and relative percentage improvements or degradations. Convergence trajectories visualized as ELO progression curves reveal learning dynamics and stability characteristics that final values alone cannot capture. Configurations that reach higher final ELO demonstrate superior ultimate capability, while those with higher average ELO show more efficient learning progression.

\subsection{Behavioral Comparison Approach}

Playstyle differentiation is evaluated by comparing tactical scores and move selection patterns between the tactical and positional clusters within each configuration. The tactical score quantifies a cluster's preference for aggressive, forcing play versus quiet strategic maneuvering. Larger differences between clusters indicate stronger behavioral specialization.

Move type analysis examines the proportion of moves classified as aggressive (captures, checks, forcing moves) versus positional (quiet moves, pawn advances, strategic improvements). Configurations with substantial differences in aggressive move percentages demonstrate that clusters have developed distinct strategic preferences aligned with their training data distributions.

Temporal stability is assessed by examining whether cluster differences remain consistent across training rounds rather than fluctuating or collapsing. Stable specialization persisting through later training indicates genuine strategic differentiation rather than transient artifacts.

\subsection{Effect Size Analysis}

To quantify the practical magnitude of observed differences, we compute Cohen's d as a standardized effect size measure:

\begin{equation}
d = \frac{\mu_1 - \mu_2}{\sigma_{\text{pooled}}}, \quad \sigma_{\text{pooled}} = \sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}}
\end{equation}

where $\mu_1$ and $\mu_2$ are the means of the two groups being compared, and $\sigma_{\text{pooled}}$ represents the pooled standard deviation.

Effect size interpretation follows standard conventions: $|d| < 0.2$ indicates negligible differences, $0.2 \leq |d| < 0.5$ represents small but noticeable effects, $0.5 \leq |d| < 0.8$ indicates medium effects with substantial practical impact, and $|d| \geq 0.8$ demonstrates large effects of considerable practical importance.

Unlike raw metric differences, Cohen's d accounts for variability in measurements, making it a more robust indicator of whether observed differences represent meaningful practical distinctions. This scale-independent measure enables comparison across different types of metrics including ELO ratings, tactical scores, and move percentages.

\subsection{Hypothesis Validation Criteria}

Each hypothesis has specific empirical criteria for validation:

\begin{itemize}
\item \textbf{H1}: Best partial sharing configuration achieves final ELO exceeding B1 final ELO, with Cohen's $d > 0.5$

\item \textbf{H2}: Partial sharing configurations exhibit parameter divergence strictly between B1 (near-zero) and B2 (maximum), satisfying $0 < \text{divergence}(E_i) < \text{divergence}(B2)$

\item \textbf{H3}: Partial sharing configurations achieve final ELO exceeding B2 final ELO

\item \textbf{H4}: Tactical and positional clusters exhibit tactical score differences with Cohen's $d > 0.8$

\item \textbf{H5}: Clusters show different aggressive move percentages with Cohen's $d > 0.5$

\item \textbf{H6}: All behavioral metrics exhibit effect sizes Cohen's $d > 0.5$

\item \textbf{H7}: Divergence metrics stabilize (plateau) and show no reconvergence in final 50 rounds
\end{itemize}

A hypothesis is considered \textbf{supported} if empirical results meet all specified criteria, \textbf{partially supported} if major criteria are met with minor exceptions, and \textbf{rejected} if key criteria fail.

\subsection{Reporting Standards}

For each comparison, results are reported using descriptive statistics (mean $M$ and standard deviation $SD$), effect sizes (Cohen's $d$ with qualitative interpretation), and both absolute and relative differences. Visual representations include line plots showing convergence trajectories, bar charts comparing final metrics, and scatter plots examining relationships between variables.

Example report format: "E2 achieves a final ELO of 1137.5 compared to B1's 1012.5, representing a 125-point improvement (12.3\% increase). Average ELO across training is 926.1 for E2 versus 896.0 for B1, a difference of 30.1 points. Cohen's $d = 0.69$ indicates a medium-to-large practical effect."

This approach emphasizes practical significance (the magnitude of differences that matter for real applications) over statistical significance based on probabilistic inference. Given the computational expense of training six complete models through 350 rounds each, the experimental design employs single runs per configuration rather than multiple random seeds, which is standard practice in deep reinforcement learning research.

\subsection{Methodological Considerations}

Several factors shape result interpretation. Each configuration is trained once rather than across multiple random initializations, limiting assessment of variance due to random factors but following standard practice for computationally expensive experiments. Metrics measured across sequential training rounds exhibit temporal dependencies, as each round builds on previous learning, so convergence trajectories represent learning dynamics rather than independent samples.

The six experimental configurations share identical infrastructure, data distributions, and hyperparameters, differing only in layer sharing policies. This controlled design enables clear attribution of performance differences to architectural choices. The analysis prioritizes effect sizes and absolute magnitudes, recognizing that a 100-point ELO difference has practical importance regardless of variance estimates, while a barely detectable 5-point difference may be statistically measurable but practically negligible.

The results chapter presents empirical findings following this methodology, enabling assessment of hypothesis validity through direct comparison of observed outcomes and their practical magnitudes.
