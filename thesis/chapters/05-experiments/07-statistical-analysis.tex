\section{Statistical Analysis}

Statistical validation ensures that observed differences between experiments are meaningful rather than artifacts of random variation. This section defines the hypothesis tests, effect size metrics, and reporting standards applied to validate the seven research hypotheses.

\subsection{Hypothesis Testing Procedures}

Different comparisons require different statistical tests based on the data structure and hypotheses. For within-configuration comparisons, we use paired t-tests to compare partial sharing configurations (P1-P4) against baselines (B1, B2) using matched pairs from the same training rounds. The null hypothesis states no difference in ELO between configurations, while the alternative proposes that partial sharing achieves higher ELO than the baseline. This validates H1 (comparing against B1) and H3 (comparing against B2).

For cluster comparisons, independent samples t-tests compare tactical cluster versus positional cluster playstyle metrics. The null hypothesis states no difference in tactical scores between clusters, while the alternative allows for any significant difference. These tests validate H4 (playstyle emergence) and H5 (distinct strategies).

Multi-configuration comparison uses one-way ANOVA with Tukey HSD post-hoc tests to simultaneously compare all P1-P4 configurations. The null hypothesis states no difference in ELO across the four partial sharing experiments, while the alternative suggests at least one configuration differs significantly. The Tukey HSD post-hoc test identifies which specific pairs differ, helping identify the optimal layer sharing strategy.

When testing multiple hypotheses on the same dataset, we apply Bonferroni correction to control family-wise error rate. The corrected significance level is $\alpha_{\text{corrected}} = 0.05 / n_{\text{tests}}$. For example, comparing four partial sharing configurations requires six pairwise comparisons, yielding $\alpha = 0.05/6 \approx 0.008$.

\subsection{Effect Size Metrics}

Statistical significance only indicates an effect exists; effect size quantifies its practical importance. For t-tests, we compute Cohen's d as:
\begin{equation}
d = \frac{\mu_1 - \mu_2}{\sigma_{\text{pooled}}}, \quad \sigma_{\text{pooled}} = \sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}}
\end{equation}

Interpretation follows standard conventions: $|d| = 0.2$ represents a small effect, $|d| = 0.5$ a medium effect, and $|d| = 0.8$ a large effect. We apply Cohen's d to cluster playstyle differences and baseline comparisons.

For ANOVA, we calculate eta-squared ($\eta^2$) to represent the proportion of variance explained:
\begin{equation}
\eta^2 = \frac{SS_{\text{between}}}{SS_{\text{total}}}
\end{equation}

Interpretation: $\eta^2 = 0.01$ indicates a small effect, $\eta^2 = 0.06$ a medium effect, and $\eta^2 = 0.14$ a large effect. This metric applies to P1-P4 configuration comparisons.

All effect sizes include 95\% confidence intervals. If the confidence interval includes zero, the effect may not be reliable despite achieving statistical significance.

\subsection{Hypothesis Validation Criteria}

Each hypothesis has specific success criteria:

\begin{itemize}
\item \textbf{H1}: Best partial sharing ELO $>$ B1 ELO, paired t-test $p < 0.05$, Cohen's $d > 0.5$
\item \textbf{H2}: $0 < \text{divergence}(P_i) < \text{divergence}(B2)$ for all partial sharing configs
\item \textbf{H3}: Partial sharing ELO $>$ B2 ELO, paired t-test $p < 0.05$
\item \textbf{H4}: Cluster tactical scores significantly different, independent t-test $p < 0.001$, Cohen's $d > 0.8$
\item \textbf{H5}: Move type distributions differ, independent t-test on aggressive move \%, $p < 0.05$, $d > 0.5$
\item \textbf{H6}: Effect sizes for behavioral metrics $> 0.5$ (Cohen's d for move type comparisons)
\item \textbf{H7}: Plateau detected (divergence stabilizes) and no reconvergence in final 50 rounds
\end{itemize}

\subsection{Reporting Standards}

For transparency and reproducibility, we report comprehensive statistics for each test. Sample sizes ($n$) are provided for each group or condition. Descriptive statistics include mean ($M$), standard deviation ($SD$), and median when distributions are skewed. Test statistics specify the $t$-value with degrees of freedom for t-tests or $F$-statistic with $df_{\text{between}}$ and $df_{\text{within}}$ for ANOVA.

Exact $p$-values are reported rather than inequality statements like "$p < 0.05$". Effect sizes include interpretation labels (small, medium, or large) based on conventional thresholds. Confidence intervals are provided at 95\% level for both mean differences and effect sizes. Each result concludes with a clear statement linking findings to the relevant hypothesis.

For example: "Tactical cluster ($M = 0.68$, $SD = 0.12$, $n = 500$) showed significantly higher tactical scores than positional cluster ($M = 0.45$, $SD = 0.11$, $n = 500$), $t(998) = 28.5$, $p < 0.001$, $d = 1.92$, 95\% CI [0.21, 0.25]. This large effect supports H4."

All statistical analyses are performed using Python (scipy.stats, statsmodels) and results are included in the results chapter with supporting visualizations.
