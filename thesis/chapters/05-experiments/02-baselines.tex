\section{Baseline Experiments}

The baseline experiments establish the performance boundaries for the partial layer sharing experiments. Two extreme configurations are tested: B1 (Full Sharing), which aggregates all layers and represents standard federated learning, and B2 (No Sharing), which runs completely independent training for each cluster. These baselines serve as control conditions to isolate the effect of selective aggregation.

\subsection{B1: Full Sharing Baseline}

The first baseline, B1 (Full Sharing), implements standard federated learning without any specialization mechanism. All model layers—input block, all 19 residual blocks, and both policy and value heads—are aggregated across clusters using the FedAvg algorithm. This configuration provides the maximum knowledge sharing between clusters but prevents any form of model specialization.

\subsubsection{Configuration}

The B1 experiment uses a shared layer configuration that includes every parameter in the model. The \texttt{shared\_layer\_patterns} list specifies all layer groups: \texttt{input\_conv.*}, \texttt{input\_bn.*}, \texttt{res\_blocks.*}, \texttt{policy\_head.*}, and \texttt{value\_head.*}. Meanwhile, the \texttt{cluster\_specific\_patterns} list remains empty, indicating that no layers are kept cluster-specific.

During aggregation, both clusters (tactical and positional) collect updates from their respective nodes and compute weighted averages using FedAvg. However, since all layers are designated as shared, the aggregation mechanism synchronizes these layers between clusters. This effectively forces both clusters to converge toward identical models, with any cluster-specific knowledge being averaged away during aggregation.

The training runs for 200 rounds, with each round consisting of local training on 400 puzzles per node followed by model aggregation. The aggregation threshold is set to 0.8, requiring at least 80\% of cluster nodes (4 out of 4 nodes, with some tolerance) to participate before aggregation occurs. The timeout per round is 1,200 seconds (20 minutes), providing sufficient time for local training, model uploads, and aggregation.

\subsubsection{Expected Behavior}

The B1 baseline represents the standard federated learning approach without clustering benefits. We expect several characteristic behaviors:

\textbf{Near-zero divergence.} Since all layers are synchronized between clusters after each round, the cluster divergence metrics should remain close to zero throughout training. Any temporary divergence that emerges during local training will be eliminated by the aggregation step. This provides a lower bound on divergence—the minimum possible separation when no specialization is allowed.

\textbf{Minimal playstyle separation.} Despite training on different puzzle sets (tactical vs. positional), the two clusters should develop similar playing styles because their models are forced to converge. The tactical score differences between clusters will be minimal, as the shared policy head cannot maintain distinct strategic preferences. This serves as a negative control for hypothesis H3, demonstrating that playstyle separation requires some form of model specialization.

\textbf{Fast convergence.} Full knowledge sharing means each cluster benefits from the training data of all 8 nodes, effectively doubling the training data size compared to independent training. We expect relatively fast convergence and good sample efficiency, with ELO ratings improving steadily across both clusters.

\textbf{Moderate performance.} The B1 baseline should achieve reasonable playing strength since it benefits from the combined training data of both clusters. However, it may underperform compared to partial sharing experiments if forcing all nodes toward a single model creates a "jack of all trades, master of none" effect. The model must compromise between tactical and positional puzzle distributions rather than specializing in either.

\subsubsection{Metrics Collection}

The B1 experiment collects comprehensive metrics to characterize model behavior. Playstyle evaluation, weight statistics, and cluster divergence are recorded every round, providing fine-grained visibility into training dynamics. Move type distribution and ELO estimation are conducted every 10 rounds to balance evaluation thoroughness with computational cost.

The playstyle evaluation generates 100 self-play games per cluster per round and computes tactical scores for middlegame positions. We expect both clusters to show similar tactical score distributions, validating that full sharing prevents specialization.

The cluster divergence computation compares weight tensors between the tactical and positional cluster models using L2 distance normalized by parameter count. The divergence is broken down by layer group (input block, early residual, middle residual, late residual, policy head, value head), though all groups should show near-zero divergence in B1.

ELO estimation plays games against Stockfish at multiple strength levels (1000, 1200, 1400 ELO) to track model performance over training. This provides a strength baseline for comparison with the partial sharing experiments.

\subsection{B2: No Sharing Baseline}

The second baseline, B2 (No Sharing), represents the opposite extreme: complete independence between clusters. No layers are aggregated between clusters, meaning the tactical and positional clusters train entirely separately using only their respective node updates. This configuration provides maximum specialization potential but zero knowledge transfer between clusters.

\subsubsection{Configuration}

The B2 configuration reverses the pattern from B1. The \texttt{shared\_layer\_patterns} list is empty, indicating that no layers are synchronized between clusters. Instead, the \texttt{cluster\_specific\_patterns} list includes all layer groups: \texttt{input\_conv.*}, \texttt{input\_bn.*}, \texttt{res\_blocks.*}, \texttt{policy\_head.*}, and \texttt{value\_head.*}.

This configuration allows each cluster to maintain its own independent model. While intra-cluster aggregation still occurs (nodes within each cluster combine their updates using FedAvg), there is no inter-cluster aggregation. The tactical cluster's model evolves based solely on tactical puzzles from its 4 nodes, while the positional cluster's model evolves based solely on positional puzzles from its 4 nodes.

The training schedule and hyperparameters match B1 exactly: 200 rounds, 400 puzzles per node per round, 0.8 aggregation threshold, and 1,200-second timeout. This ensures that any performance differences between B1 and B2 can be attributed to the sharing policy rather than confounding variables.

\subsubsection{Expected Behavior}

The B2 baseline tests whether cluster specialization is beneficial when taken to the extreme. We expect several contrasting behaviors compared to B1:

\textbf{High divergence.} With no cross-cluster aggregation, the tactical and positional models will evolve along completely different trajectories. Divergence should increase steadily during early training as each model adapts to its specific puzzle distribution. By the end of training, we expect divergence values significantly higher than B1 across all layer groups, providing an upper bound on model separation.

\textbf{Maximum playstyle separation.} The B2 configuration provides ideal conditions for behavioral differentiation. The tactical cluster should develop a highly tactical playing style with high tactical scores, frequent captures and checks, and aggressive move preferences. The positional cluster should develop a positional playing style with lower tactical scores, more quiet moves, and focus on long-term planning. This serves as a positive control for hypothesis H3, demonstrating that specialization is possible when models train independently.

\textbf{Slower convergence.} Unlike B1, each cluster only has access to its own 4 nodes' data (1,600 puzzles per round total, compared to 3,200 when sharing with the other cluster). This smaller effective dataset may slow convergence and require more rounds to reach comparable performance. The learning curves should show slower ELO improvement compared to configurations with knowledge sharing.

\textbf{Potentially lower performance.} While specialization allows each cluster to optimize for its specific puzzle distribution, the lack of knowledge transfer means each cluster cannot benefit from patterns learned by the other cluster. General chess knowledge (like piece values, board control, king safety) that applies to both tactical and positional puzzles must be learned independently by each cluster rather than shared. This may result in lower final ELO compared to partial sharing experiments that combine specialization with knowledge transfer.

\subsubsection{Metrics Collection}

The B2 experiment uses the same metrics collection protocol as B1, ensuring fair comparison. The key difference is in the interpretation of metrics rather than their collection.

Playstyle evaluation in B2 should reveal clear separation between clusters. We expect the tactical cluster's mean tactical score to be significantly higher than the positional cluster's mean tactical score, with non-overlapping distributions and large effect sizes.

Cluster divergence serves as a key indicator of specialization success. Unlike B1 where divergence should be near-zero, B2's divergence should grow over training and stabilize at high values. The divergence breakdown by layer group reveals which layers specialize most strongly—we expect all layers to show high divergence since no sharing occurs.

Move type distribution analysis should show distinct behavioral signatures. The tactical cluster should play more captures, checks, and aggressive moves, while the positional cluster should favor quiet moves, pawn advances, and positional maneuvering. These differences validate that training on different puzzle sets produces measurably different playing styles.

\subsection{Baseline Comparison}

The two baselines represent opposite ends of a spectrum: full sharing versus no sharing. Comparing their performance and behavior motivates the partial sharing experiments in Phase 2.

\subsubsection{Divergence and Specialization}

The most obvious difference between B1 and B2 is cluster divergence. B1 should maintain near-zero divergence throughout training because all layers are synchronized after each round. Any divergence that emerges during local training (due to different puzzle distributions) is eliminated by aggregation. In contrast, B2 should show steadily increasing divergence during early training as each cluster's model adapts to its specific data distribution, eventually plateauing once specialization is complete.

This divergence difference validates hypothesis H2 from a boundary conditions perspective: B1 shows that sharing all layers prevents specialization (divergence $\approx$ 0), while B2 shows that sharing no layers allows maximum specialization (high divergence). The partial sharing experiments will test whether intermediate divergence levels can be achieved by selectively sharing some layers while keeping others cluster-specific.

\subsubsection{Performance and Knowledge Transfer}

The performance comparison between B1 and B2 tests hypothesis H5 (cross-cluster learning enables knowledge transfer). If B1 significantly outperforms B2, it suggests that knowledge sharing is beneficial despite preventing specialization. Conversely, if B2 matches or exceeds B1's performance, it suggests that specialization benefits outweigh knowledge transfer benefits, motivating stronger specialization in the partial sharing experiments.

We expect the truth to lie somewhere in between: B1 may show faster initial convergence due to larger effective dataset size, but B2 may achieve higher final performance on cluster-specific tasks due to specialization. The partial sharing experiments aim to capture the best of both worlds—fast convergence through shared general knowledge and strong final performance through specialized cluster-specific layers.

\subsubsection{Behavioral Differentiation}

The behavioral metrics provide the clearest contrast between baselines. B1 should show minimal playstyle differences between clusters because the shared policy head cannot maintain distinct move preferences. Both clusters will converge toward a compromise playing style that balances tactical and positional considerations.

B2, on the other hand, should show maximum behavioral differentiation. With completely independent models, each cluster can fully optimize for its training distribution. The tactical cluster should exhibit highly tactical play, while the positional cluster should exhibit strongly positional play. The effect sizes for tactical score differences and move type distribution differences should be large in B2 but small in B1.

This comparison validates hypothesis H3 (playstyle clusters emerge naturally) by demonstrating that behavioral differentiation requires model specialization. It also provides guidance for the partial sharing experiments: if we want clusters to develop distinct playing styles, we must keep at least some layers (likely the policy head) cluster-specific rather than shared.

\subsubsection{Design Space Motivation}

The baseline comparison motivates the design space explored in Phase 2. B1 and B2 represent extreme points: sharing all layers or sharing no layers. The performance and behavioral characteristics of these extremes inform which intermediate configurations are worth testing.

If B1 converges faster but achieves lower final performance, it suggests that early layers (which learn general features) should be shared to accelerate learning, while late layers (which make final decisions) should remain cluster-specific for specialization. This motivates experiment P4 (share all except heads).

If B2 shows strong specialization but slow convergence, it suggests that some knowledge transfer is beneficial but full sharing is excessive. This motivates experiments P1-P3, which selectively share different layer groups to balance knowledge transfer and specialization.

The baselines also establish performance bounds for statistical testing. Hypothesis H1 requires that partial sharing outperforms B1 (full sharing), while hypothesis H5 requires that partial sharing outperforms B2 (no sharing). Together, these constraints define success: the optimal partial sharing configuration should beat both baselines by achieving stronger final performance than B1 while converging faster than B2.
