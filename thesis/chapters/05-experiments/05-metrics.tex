\section{Evaluation Metrics}
\label{sec:evaluation-metrics}

Our evaluation framework measures three distinct aspects of the federated learning system: playing strength, playstyle preservation, and cluster divergence. These metrics assess whether selective aggregation achieves the dual objectives of maintaining competitive playing ability while preserving distinct tactical and positional characteristics. This section describes the metrics, their collection schedule, and expected patterns across different experimental configurations.

\subsection{Playing Strength Metrics}

Playing strength is quantified through ELO rating estimation based on match results against calibrated Stockfish opponents at multiple difficulty levels (1000, 1200, and 1400 ELO). For each cluster, we play a series of evaluation matches with alternating colors, yielding 30 total evaluation games per cluster per evaluation round.

ELO estimation uses the standard formula where the expected score between two players is:

\begin{equation}
E(R_{\text{test}}, R_{\text{opp}}) = \frac{1}{1 + 10^{(R_{\text{opp}} - R_{\text{test}})/400}}
\end{equation}

We estimate the cluster's ELO rating by finding the rating that best fits the observed match results across all opponent levels using least squares optimization. Confidence intervals are computed based on the number of evaluation games, with our default of 30 games providing a $\pm 100$ ELO confidence range.

\subsection{Playstyle Metrics}

Playstyle characterization quantifies the tactical versus positional nature of each cluster's play through comprehensive analysis of self-play and evaluation games. The primary metric is the \textbf{tactical score}, a normalized composite metric ranging from 0.0 (purely positional) to 1.0 (purely tactical).

The tactical score integrates three normalized component metrics:

\textbf{Attacks metric}: Measures the total material value of opponent pieces under attack, normalized by the maximum possible attacked material (39 points).

\textbf{Moves metric}: Measures the average number of legal moves available, normalized by typical middlegame move counts (40 moves).

\textbf{Material metric}: Measures total material captured during the opening and early middlegame, normalized by significant exchange thresholds (20 points).

These components are combined through weighted averaging:

\begin{equation}
\text{TacticalScore} =
\begin{cases}
\frac{\text{AttacksMetric} + \text{MovesMetric} + \text{MaterialMetric}}{3} & \text{if MaterialMetric} > 0 \\[0.3cm]
\frac{\text{AttacksMetric} + \text{MovesMetric}}{2} & \text{otherwise}
\end{cases}
\end{equation}

For each cluster, we report the mean tactical score, standard deviation, and distribution across five classification categories: Very Tactical ($> 0.70$), Tactical ($0.65$-$0.70$), Balanced ($0.60$-$0.65$), Positional ($0.50$-$0.60$), and Very Positional ($< 0.50$).

Beyond the aggregate tactical score, we perform detailed \textbf{move-level classification} to quantify specific playing patterns. Each move is classified into categories including captures, checks, pawn advances, quiet moves, and aggressive moves (union of captures and checks). We compute the percentage of moves in each category, enabling both absolute and relative comparisons between clusters.

\subsection{Cluster Divergence Metrics}

Cluster divergence quantifies the degree to which clusters have developed distinct internal representations. We measure divergence at both the parameter level (comparing neural network weights) and behavioral level (comparing playstyle metrics).

\subsubsection{Parameter-Level Divergence}

Parameter-level divergence compares neural network weights between cluster models on a layer-by-layer basis. For each layer group, we compute the L2 distance between weight tensors, normalized by parameter count to enable fair comparison across layers of different sizes:

\begin{equation}
\text{Divergence}(W_A, W_B) = \frac{\|W_A - W_B\|_2}{\sqrt{|\text{Parameters}|}}
\end{equation}

We aggregate results by layer group (input block, early residual blocks, middle residual blocks, late residual blocks, policy head, value head). Under selective aggregation, we expect low divergence in shared layer groups and higher divergence in cluster-specific groups.

\subsubsection{Behavioral Divergence}

Behavioral divergence measures differences in playstyle metrics between clusters. Key metrics include:

\textbf{Playstyle divergence}: The absolute difference in mean tactical scores between clusters, quantifying behavioral separation.

\textbf{ELO spread}: The range of playing strengths across clusters, indicating whether selective aggregation creates strength imbalances.

\textbf{Move type differences}: Absolute differences in move category percentages (captures, checks, quiet moves) between clusters, validating stylistic separation.

\subsection{Metric Collection Schedule}

Metrics are collected at two different frequencies to balance evaluation thoroughness with computational cost. Every training round from 1 to 200, we collect playstyle evaluations by generating 100 self-play games per cluster and analyzing tactical scores. Weight statistics provide a complete parameter analysis including mean, standard deviation, sparsity, and dead neuron counts. Cluster divergence is measured through pairwise L2 distance comparisons between cluster models, and training loss is averaged across local client updates.

Every 10 rounds (at rounds 10, 20, 30, through 200), we perform more computationally expensive evaluations. Move type distribution analysis classifies all moves from recent games into categories like captures, checks, quiet moves, and pawn advances. ELO estimation runs 30 games against Stockfish opponents at three difficulty levels (1000, 1200, and 1400 ELO) to approximate playing strength. This schedule ensures fine-grained tracking of playstyle and divergence evolution while limiting the computational overhead of detailed analysis.

\subsection{Expected Metric Patterns}

Different experimental configurations should produce characteristic metric patterns that validate our hypotheses. The full sharing baseline (B1) should exhibit near-zero cluster divergence (below 0.01) across all layer groups due to complete parameter synchronization. Tactical score differences between clusters should be minimal (under 0.05) despite different training data, and ELO ratings should reach moderate strength levels due to access to all eight nodes' combined data.

The no sharing baseline (B2) represents the opposite extreme. Cluster divergence should be high (above 0.3) across all layer groups due to completely independent training. Tactical score differences should be maximal (exceeding 0.15), indicating strong behavioral separation. ELO may be potentially lower since each cluster only accesses four nodes' worth of data. The divergence pattern should show high values uniformly across input layers, residual blocks, and policy/value heads.

The early sharing configuration (P1) should display low divergence (below 0.05) in input and early residual blocks, while late residual and heads show high divergence (above 0.2). Tactical score differences should fall in the moderate to high range (0.10 to 0.15). ELO performance should be strong due to shared low-level features combined with specialized high-level reasoning.

The backbone sharing configuration (P4) was hypothesized to achieve optimal performance. It should show low divergence (under 0.05) in all residual blocks while policy and value heads exhibit very high divergence (exceeding 0.4). If heads alone can encode playstyle, tactical score differences should be high (above 0.15). The predicted optimal ELO stems from maximal knowledge sharing across 86\% of parameters. The divergence pattern should show clear separation with near-zero divergence in the backbone and concentrated divergence in the 14\% of parameters comprising the heads.

The middle and late sharing configurations (P2 and P3) serve as exploratory experiments. P2 shares only middle layers and should demonstrate moderate divergence in both input/early layers and policy/value heads. P3 shares late layers plus heads and is expected to fail at playstyle separation since shared policy heads cannot maintain distinct move preferences---this configuration tests whether forcing convergence in decision-making layers prevents behavioral specialization regardless of earlier layer independence.

\subsection{Hypothesis-Metric Mapping}

Each research hypothesis maps to specific metrics and validation criteria. H1 tests whether clustered federated learning outperforms centralized training by comparing P4 ELO against B1 ELO. H2 validates controlled specialization by verifying that partial sharing configurations achieve divergence values strictly between B1's near-zero and B2's maximum divergence. H3 tests cross-cluster learning benefits by comparing P4 ELO against B2 ELO to validate whether knowledge transfer through shared layers provides performance advantages.

H4 examines playstyle emergence through statistical tests on tactical score differences between clusters, expecting highly significant results with $p < 0.001$ and large effect sizes with Cohen's $d > 0.8$. H5 investigates distinct strategic preferences by comparing move type distributions, particularly aggressive move percentages, between tactical and positional clusters. H6 validates measurability by requiring effect sizes exceeding 0.5 for all behavioral metric comparisons, ensuring practical significance accompanies statistical significance. H7 assesses training stability by applying plateau detection algorithms to divergence and tactical score trajectories, verifying that specialization maintains without reconvergence.

The complete statistical validation procedures, including hypothesis tests and effect size calculations, are described in Section 5.7.
