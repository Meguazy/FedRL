\section{Training Configuration}

All experiments use identical training configurations to ensure fair comparison, with the only differences being the layer sharing policies (B1, B2, E1-E4). This section specifies the hyperparameters, data configuration, and model architecture used throughout the experimental evaluation.

\subsection{Model Architecture}

All experiments use the AlphaZero-style convolutional residual network described in Section~\ref{sec:network-architecture}. The architecture consists of an input block (single convolutional layer with batch normalization processing the 119-plane board representation), 19 residual blocks (each with two $3 \times 3$ convolutional layers, 256 filters, batch normalization, and skip connections), a policy head (convolutional layer and fully connected layer outputting 4,672 move logits), and a value head (convolutional layer and fully connected layers outputting a scalar position evaluation in [-1, 1]).

The model contains approximately 11 million parameters: 20K in the input block, 9.5M in residual blocks, and 2M in the heads. This configuration provides sufficient capacity for learning chess at advanced amateur level while remaining computationally feasible for federated training. The architecture is identical across all experiments, only the aggregation strategy differs.

\subsection{Training Hyperparameters}

Each experiment runs for 350 training rounds, with each round consisting of local client training followed by model aggregation according to the experiment's sharing policy.

\textbf{Federated learning parameters:}
\begin{itemize}
\item Total rounds: 350
\item Nodes per cluster: 4 tactical + 4 positional (8 total)
\item Games per node per round: 400 (3,200 games total per round)
\item Local training epochs: 5 epochs over the 400 games before aggregation
\item Aggregation threshold: 0.8 (80\% of cluster nodes must participate)
\item Round timeout: 1,200 seconds (20 minutes)
\end{itemize}

The aggregation threshold of 0.8 requires at least 80\% of cluster nodes to participate (with some tolerance for occasional node failures). If fewer nodes participate in a round, aggregation is skipped and models retain their previous weights.

\textbf{Neural network optimization:}
\begin{itemize}
\item Optimizer: Adam with default parameters ($\beta_1 = 0.9$, $\beta_2 = 0.999$)
\item Learning rate: $1 \times 10^{-3}$ (constant, no decay schedule)
\item Batch size: 64 games
\item Weight decay: $1 \times 10^{-4}$ for L2 regularization
\item Gradient clipping: Not used (Adam's adaptive learning rate provides sufficient stability)
\end{itemize}

\textbf{Loss function:}
The training objective combines policy loss and value loss with equal weights:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{policy}} + \mathcal{L}_{\text{value}}
\end{equation}

where $\mathcal{L}_{\text{policy}}$ is the cross-entropy loss between the predicted move distribution and the target move, and $\mathcal{L}_{\text{value}}$ is the mean squared error between the predicted position value and the game outcome (1.0 for wins, 0.0 for losses, 0.5 for draws).

\subsection{Data Configuration}

Training data comes from the Lichess game database, a collection of millions of rated chess games. Games are filtered by ECO opening codes into disjoint tactical and positional sets as described in Section~\ref{sec:playstyle-filtering}.

The tactical cluster trains on games from sharp tactical openings emphasizing dynamic imbalances and concrete calculations, including the Sicilian Defence (B20-B99) with variations like the Dragon and Najdorf, the King's Gambit (C30-C39), Italian Game aggressive lines (C50-C54), Alekhine's Defence (B02-B05), and Vienna Game (C25-C29). The positional cluster trains on games from strategic positional openings emphasizing structural advantages and piece coordination, including Queen's Gambit Declined (D30-D69), Slav Defence (D10-D19), Nimzo-Indian Defence (E20-E59), Queen's Indian Defence (E12-E19), Catalan Opening (E00-E09), English Opening (A10-A39), and RÃ©ti Opening (A04-A09).

The game sets are completely disjoint by ECO code, no opening classification appears in both the tactical and positional sets. This ensures that behavioral differences emerge from the training distribution rather than from shared games.

Each node samples 400 games per round from its cluster's filtered game set using deterministic offset-based sampling. Over 350 rounds, each node trains on 140,000 game positions, providing sufficient data for convergence. Data augmentation is applied through board flips (horizontal reflection with move translation) and color swaps (playing from black's perspective), effectively doubling the training data size and improving generalization to symmetrically equivalent positions.

Each game is converted into training examples by extracting positions and moves from the game record, creating a 119-plane tensor representation of each position (Section~\ref{sec:network-architecture}) as input, the actual move played as the policy target, and the game outcome as the value target. This supervised learning approach trains the network to predict expert-level moves in games exhibiting tactical versus positional characteristics, providing a foundation for subsequent self-play reinforcement learning if desired.
