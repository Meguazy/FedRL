\chapter{Methodology}

This chapter presents our clustered federated deep reinforcement learning framework for chess with selective layer aggregation. We begin by formally defining the problem as an extension of the standard reinforcement learning formulation, incorporating the unique challenges of maintaining strategic diversity in a federated setting. We then describe the three-tier hierarchical aggregation system that enables knowledge transfer across clusters while preserving playstyle-specific characteristics. The chapter proceeds to detail the neural network architecture, the selective aggregation mechanism, the playstyle-aware data filtering strategy, and the complete training pipeline from supervised bootstrapping through self-play reinforcement learning. Finally, we present the evaluation methodology that will be used to validate our approach in Chapter 5.

\input{chapters/03-methodology/01-problem-formulation}
\input{chapters/03-methodology/02-clustered-framework}
\input{chapters/03-methodology/03-neural-architecture}
\input{chapters/03-methodology/04-aggregation-system}
\input{chapters/03-methodology/05-selective-aggregation}
\input{chapters/03-methodology/06-data-filtering}
\input{chapters/03-methodology/07-training-procedures}
\input{chapters/03-methodology/08-evaluation}
