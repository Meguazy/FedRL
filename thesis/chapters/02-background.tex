\chapter{Background and Related Work}

This chapter provides the theoretical foundation for our clustered federated deep reinforcement learning framework. We begin by describing our literature search methodology to ensure transparency and reproducibility. We then provide an overview of reinforcement learning fundamentals and the AlphaZero algorithm that forms the basis of our chess engine. We introduce federated learning principles and discuss how they can be adapted to reinforcement learning settings. Finally, we review related work in distributed reinforcement learning, behavioral diversity preservation, and federated learning applications.

\section{Literature Review Methodology}

To ensure a comprehensive and systematic review of relevant literature, we conducted a multi-stage search process across academic databases, preprint repositories, and technical documentation. This section details our search strategy, inclusion criteria, and the tools used to identify and synthesize relevant work.

\subsection{Search Strategy}

We performed systematic searches across multiple academic databases and repositories between September 2024 and January 2025. The primary sources included:

\begin{itemize}
    \item \textbf{Google Scholar:} Broad coverage of computer science literature and citation tracking
    \item \textbf{arXiv.org:} Recent preprints in machine learning (cs.LG, cs.AI, cs.MA)
    \item \textbf{ACM Digital Library:} Conference proceedings (NeurIPS, ICML, ICLR, AAAI)
    \item \textbf{IEEE Xplore:} Systems and distributed computing literature
    \item \textbf{Semantic Scholar:} AI-powered search and paper recommendations
\end{itemize}

\subsection{Search Terms and Queries}

Our literature search employed combinations of core terms, connected with Boolean operators. Table~\ref{tab:search-queries} shows the primary and secondary search queries used across different databases.

\begin{table}[h]
\centering
\caption{Literature Search Queries}
\label{tab:search-queries}
\begin{tabular}{|p{3cm}|p{10cm}|}
\hline
\textbf{Category} & \textbf{Search Query} \\
\hline
\multicolumn{2}{|l|}{\textbf{Primary Queries}} \\
\hline
Federated RL & "federated learning" AND "reinforcement learning" \\
\hline
Clustered FL & "clustered federated learning" OR "personalized federated learning" \\
\hline
Distributed Chess AI & "AlphaZero" AND ("federated" OR "distributed") \\
\hline
Behavioral Diversity & "behavioral diversity" AND "multi-agent" \\
\hline
Chess Playstyle & "chess AI" AND ("playing style" OR "playstyle") \\
\hline
Selective Aggregation & "selective aggregation" AND "federated learning" \\
\hline
Distributed MCTS & "Monte Carlo Tree Search" AND "distributed" \\
\hline
\multicolumn{2}{|l|}{\textbf{Secondary Queries}} \\
\hline
Heterogeneous FL & "heterogeneous federated learning" \\
\hline
Non-IID Data & "non-IID federated learning" \\
\hline
Transfer Learning & "transfer learning" AND "deep reinforcement learning" \\
\hline
Distributed Self-Play & "self-play" AND ("distributed" OR "federated") \\
\hline
Quality Diversity & "quality diversity algorithms" \\
\hline
Model Divergence & "model divergence" AND "federated" \\
\hline
\end{tabular}
\end{table}

We also performed backward citation tracking (reviewing references of key papers) and forward citation tracking (identifying papers that cite foundational work) to ensure coverage of relevant literature.

\subsection{Inclusion and Exclusion Criteria}

Papers were included if they met the following criteria:

\textbf{Inclusion criteria:}
\begin{itemize}
    \item Published between 2015 and 2025 (with exceptions for seminal earlier work)
    \item Directly relevant to federated learning, reinforcement learning, or chess AI
    \item Peer-reviewed or from reputable preprint repositories (arXiv)
    \item Available in English
    \item Sufficient technical detail to understand methodology
\end{itemize}

\textbf{Exclusion criteria:}
\begin{itemize}
    \item Purely theoretical work without implementation insights
    \item Domain-specific applications unrelated to game-playing or multi-agent systems
    \item Duplicate publications or superseded versions
    \item Insufficient detail on methods or results
\end{itemize}

\subsection{AI-Assisted Literature Discovery}

In addition to traditional database searches, we leveraged AI tools to assist with literature discovery and synthesis. Table~\ref{tab:ai-queries} shows the AI-assisted queries used for research assistance.

\begin{table}[h]
\centering
\caption{AI-Assisted Literature Discovery Queries}
\label{tab:ai-queries}
\begin{tabular}{|p{4cm}|p{9cm}|}
\hline
\textbf{Tool} & \textbf{Query / Purpose} \\
\hline
\multicolumn{2}{|l|}{\textbf{Claude (Anthropic)}} \\
\hline
Federated RL Overview & "Summarize recent advances in federated reinforcement learning, focusing on methods that handle heterogeneous agents" \\
\hline
Behavioral Diversity & "What are the key challenges in maintaining behavioral diversity in multi-agent systems?" \\
\hline
Selective Aggregation & "Compare different approaches to selective aggregation in federated learning" \\
\hline
Distributed AlphaZero & "Find papers that combine AlphaZero-style training with distributed or federated approaches" \\
\hline
\multicolumn{2}{|l|}{\textbf{Semantic Scholar}} \\
\hline
Recommendations & AI-powered paper recommendations based on citation graphs and content similarity \\
\hline
\multicolumn{2}{|l|}{\textbf{Connected Papers}} \\
\hline
Citation Networks & Visualizing citation networks and identifying research clusters \\
\hline
\end{tabular}
\end{table}

These AI-assisted searches were particularly useful for:
\begin{enumerate}
    \item Quickly understanding the landscape of a new research area
    \item Identifying terminology variations (e.g., "behavioral diversity" vs "policy diversity" vs "strategic heterogeneity")
    \item Discovering connections between seemingly disparate research communities (e.g., federated learning and chess AI)
    \item Generating additional search terms based on paper abstracts
\end{enumerate}

\subsection{Documentation and Synthesis}

We maintained a structured database of reviewed papers using reference management software, tracking:
\begin{itemize}
    \item Paper metadata (authors, venue, year)
    \item Key contributions and findings
    \item Methodological approaches
    \item Relevance to our research questions
    \item Gaps or limitations identified
\end{itemize}

This systematic approach ensured comprehensive coverage of relevant literature while maintaining focus on our core research questions about clustered federated learning for reinforcement learning with behavioral diversity preservation.

\section{Reinforcement Learning}

Reinforcement learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. Unlike supervised learning, where correct answers are provided, RL agents must discover effective strategies through trial and error, receiving only sparse feedback about the quality of their actions.

\subsection{Markov Decision Processes}

Reinforcement learning problems are typically formalized as Markov Decision Processes (MDPs). An MDP is defined by a tuple $(S, A, P, R, \gamma)$ where:

\begin{itemize}
    \item $S$ is the set of possible states the environment can be in
    \item $A$ is the set of actions the agent can take
    \item $P(s'|s,a)$ is the transition probability of reaching state $s'$ after taking action $a$ in state $s$
    \item $R(s,a,s')$ is the reward received when transitioning from state $s$ to $s'$ via action $a$
    \item $\gamma \in [0,1]$ is the discount factor that determines how much future rewards are valued relative to immediate rewards
\end{itemize}

The agent's behavior is determined by a policy $\pi(a|s)$ that specifies the probability of taking action $a$ in state $s$. The goal of reinforcement learning is to find an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward, known as the return:

\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

The value function $V^\pi(s)$ represents the expected return when starting in state $s$ and following policy $\pi$:

\begin{equation}
    V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\end{equation}

Similarly, the action-value function $Q^\pi(s,a)$ represents the expected return when taking action $a$ in state $s$ and then following policy $\pi$:

\begin{equation}
    Q^\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
\end{equation}

\subsection{Deep Reinforcement Learning}

Traditional RL algorithms use tabular representations to store value functions, which becomes impractical for large state spaces. Deep reinforcement learning addresses this limitation by using neural networks as function approximators to estimate value functions and policies. This enables RL to scale to complex domains like video games, robotics, and board games.

Deep Q-Networks (DQN) pioneered this approach by using convolutional neural networks to approximate the action-value function $Q(s,a)$ for Atari games. The key innovations included experience replay, where transitions are stored in a buffer and sampled randomly for training, and a separate target network that stabilizes learning.

Policy gradient methods provide an alternative approach by directly parameterizing the policy $\pi_\theta(a|s)$ with neural network parameters $\theta$. The policy gradient theorem allows us to compute gradients of the expected return with respect to these parameters:

\begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]
\end{equation}

Actor-critic methods combine these approaches by maintaining both a policy network (actor) and a value network (critic). The critic evaluates the quality of the actor's actions, providing lower-variance gradient estimates.

\subsection{AlphaZero and Monte Carlo Tree Search}

AlphaZero represents a breakthrough in deep reinforcement learning for board games, achieving superhuman performance in chess, Go, and shogi through pure self-play learning without human knowledge. The algorithm combines three key components: a deep neural network for position evaluation, Monte Carlo Tree Search for move planning, and reinforcement learning for continuous improvement.

\subsubsection{Neural Network Architecture}

The AlphaZero neural network takes the current board position as input and produces two outputs:
\begin{itemize}
    \item A \textbf{policy head} $p = f_\theta^p(s)$ that outputs a probability distribution over legal moves
    \item A \textbf{value head} $v = f_\theta^v(s)$ that outputs a scalar value estimating the probability of winning from the current position
\end{itemize}

The network uses a deep residual architecture with convolutional layers to process spatial patterns on the board. This dual-headed design allows the network to both suggest promising moves and evaluate position quality, which are used together during search.

\subsubsection{Monte Carlo Tree Search}

Monte Carlo Tree Search (MCTS) is a best-first search algorithm that builds a search tree incrementally through random sampling. Unlike traditional minimax search used in classical chess engines, MCTS focuses computational effort on the most promising variations.

Each node in the search tree represents a board position and stores statistics about visits and values. The search proceeds through four phases:

\begin{enumerate}
    \item \textbf{Selection:} Starting from the root, choose child nodes that balance exploration (trying less-visited moves) and exploitation (following moves with high estimated value) using the PUCT formula:
    \begin{equation}
        UCT(s,a) = Q(s,a) + c_{puct} P(s,a) \frac{\sqrt{\sum_b N(s,b)}}{1 + N(s,a)}
    \end{equation}
    where $Q(s,a)$ is the mean action value, $P(s,a)$ is the prior probability from the neural network, and $N(s,a)$ is the visit count.

    \item \textbf{Expansion:} When a leaf node is reached, evaluate the position using the neural network to get policy priors and value estimate.

    \item \textbf{Simulation:} In AlphaZero, this phase is replaced by direct neural network evaluation rather than random rollouts.

    \item \textbf{Backpropagation:} Update statistics along the path from leaf to root, incrementing visit counts and updating action values.
\end{enumerate}

After running many MCTS simulations (typically 800 for AlphaZero), the final move is selected based on visit counts, which represent a refined estimate of move quality informed by deep search.

\subsubsection{Self-Play Training}

AlphaZero improves through iterative self-play. The current neural network generates training games by playing against itself using MCTS-guided move selection. Each position in these games provides training data:

\begin{itemize}
    \item The \textbf{policy target} is the distribution of MCTS visit counts $\pi$, which represents an improved policy compared to the raw network output
    \item The \textbf{value target} is the final game outcome $z \in \{-1, 0, 1\}$ (loss, draw, win)
\end{itemize}

The network is trained to minimize a combined loss function:

\begin{equation}
    L = (z - v)^2 - \pi^T \log p + c||\theta||^2
\end{equation}

This loss function encourages the network to predict game outcomes accurately (value loss) and match the improved MCTS policy (policy loss), with L2 regularization to prevent overfitting.

The key insight of AlphaZero is that MCTS can be viewed as a policy improvement operator. By repeatedly training the network on self-play games where moves are selected by MCTS, the network gradually improves, which in turn makes future MCTS searches more effective. This creates a positive feedback loop that leads to continuous improvement without requiring any domain knowledge beyond the game rules.

\section{Federated Learning}

Federated learning is a distributed machine learning paradigm that enables multiple participants to collaboratively train a shared model while keeping their data decentralized. Unlike traditional centralized training where all data is aggregated in one location, federated learning brings the model to the data rather than the data to the model. This approach addresses privacy concerns, reduces communication costs, and enables learning from data that cannot be easily centralized due to legal, technical, or practical constraints.

\subsection{Federated Averaging Algorithm}

The foundational algorithm for federated learning is Federated Averaging (FedAvg), proposed by McMahan et al. FedAvg coordinates distributed training across multiple clients through a central server that aggregates local model updates.

The basic FedAvg procedure consists of several rounds of communication between the server and clients:

\begin{enumerate}
    \item \textbf{Model Distribution:} The server sends the current global model parameters $w_t$ to a subset of clients

    \item \textbf{Local Training:} Each selected client $k$ trains the model on their local dataset $D_k$ for $E$ epochs, producing updated parameters $w_t^k$

    \item \textbf{Aggregation:} The server collects the updated models and computes a weighted average:
    \begin{equation}
        w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_t^k
    \end{equation}
    where $n_k = |D_k|$ is the size of client $k$'s dataset and $n = \sum_k n_k$ is the total data size

    \item \textbf{Iteration:} This process repeats for multiple rounds until convergence
\end{enumerate}

The key advantage of FedAvg is that it performs multiple local optimization steps before communication, significantly reducing the number of communication rounds needed compared to sending gradients after each batch. This is crucial since communication is often the bottleneck in distributed settings.

\subsection{Challenges in Federated Learning}

While federated learning offers many benefits, it also introduces several challenges compared to centralized training:

\textbf{Non-IID Data:} In typical federated settings, data across clients is not independently and identically distributed. Different clients may have data from different distributions, different label distributions, or different amounts of data. This heterogeneity can slow convergence and lead to suboptimal models.

\textbf{Communication Efficiency:} Since clients may have limited bandwidth or intermittent connectivity, minimizing communication rounds is essential. Techniques like gradient compression, quantization, and local optimization help reduce communication costs.

\textbf{System Heterogeneity:} Clients may have varying computational capabilities, storage capacity, and availability. Some clients may be able to train quickly on powerful hardware while others are limited by mobile device constraints.

\textbf{Privacy and Security:} While federated learning keeps raw data decentralized, model updates can still leak information about training data. Differential privacy and secure aggregation techniques can provide stronger privacy guarantees but add computational overhead.

\subsection{Personalization in Federated Learning}

A key limitation of vanilla FedAvg is its assumption that all clients should converge to a single global model. However, in many real-world scenarios, different clients or groups of clients may benefit from specialized models tailored to their specific data distributions or preferences.

Personalized federated learning addresses this by allowing some degree of model customization per client or client group while still leveraging collaborative learning. Several approaches have been proposed:

\textbf{Fine-tuning:} Clients start with a global model but continue training locally after federation completes, adapting the model to their specific data.

\textbf{Multi-task Learning:} The model is split into shared and personalized layers. Shared layers learn common representations across all clients, while personalized layers adapt to individual client characteristics.

\textbf{Clustered Federated Learning:} Clients are grouped into clusters based on data similarity or other criteria. Each cluster maintains its own model through federated averaging within the cluster, while potentially sharing knowledge across clusters.

\textbf{Meta-Learning:} The global model is trained to be easily adaptable to new clients with minimal fine-tuning, using techniques like Model-Agnostic Meta-Learning (MAML).

These personalization techniques recognize that a one-size-fits-all global model is not always optimal, especially when client data distributions are significantly different. Our work builds on the clustered federated learning approach, extending it to reinforcement learning settings where behavioral diversity is not just a result of data heterogeneity but a desired outcome.

\section{Chess Engines and AI}

Computer chess has been a central domain for artificial intelligence research since the field's inception. The evolution of chess engines reflects broader trends in AI, from symbolic rule-based systems to search algorithms to modern deep learning approaches.

\subsection{Classical Chess Engines}

Traditional chess engines rely on three core components: board representation, move generation, and position evaluation combined with tree search.

\textbf{Minimax and Alpha-Beta Pruning:} Classical engines use minimax search to explore the game tree, assuming both players play optimally. The algorithm recursively evaluates positions by assuming the maximizing player wants the highest score while the minimizing player wants the lowest. Alpha-beta pruning dramatically reduces the search space by eliminating branches that cannot affect the final decision.

\textbf{Hand-Crafted Evaluation Functions:} Classical engines evaluate positions using carefully designed functions that consider material balance, piece activity, pawn structure, king safety, and other strategic factors. These evaluation functions encode centuries of human chess knowledge into numerical scores.

Stockfish, currently the strongest classical chess engine, represents the pinnacle of this approach. It combines sophisticated search algorithms, aggressive pruning techniques, and finely tuned evaluation heuristics to search billions of positions per second. Despite being based on traditional methods, Stockfish remains competitive with neural network engines in many positions.

\subsection{Neural Network Chess Engines}

The introduction of AlphaZero in 2017 demonstrated that neural networks trained through self-play could achieve superhuman chess performance without domain knowledge. Unlike classical engines that use hand-crafted evaluation functions, AlphaZero learned position evaluation and move selection entirely from self-play.

The success of AlphaZero inspired several open-source projects, most notably Leela Chess Zero (LC0), which reimplemented the AlphaZero approach using distributed training across thousands of volunteer computers. LC0 has evolved to match and sometimes exceed Stockfish's playing strength, particularly in positions requiring long-term strategic planning.

Neural network engines exhibit different playing characteristics compared to classical engines. They tend to favor positional understanding and long-term planning over tactical calculation depth. This has enriched computer chess by introducing more varied and sometimes more human-like playing styles.

\subsection{Playing Style in Chess}

Chess players, both human and computer, exhibit distinct playing styles that reflect different strategic philosophies. Two broad categories often used to characterize playing style are:

\textbf{Tactical Play:} Emphasizes concrete calculation, immediate threats, and combinative play. Tactical players excel at spotting forcing sequences, sacrifices, and sharp variations. They prefer dynamic positions with many pieces on the board where calculation depth determines the outcome.

\textbf{Positional Play:} Focuses on long-term strategic advantages like pawn structure, piece coordination, and space control. Positional players excel at gradual maneuvering, prophylaxis, and converting small advantages into wins. They prefer positions where understanding trumps calculation.

In human chess, players typically develop preferences for certain opening systems and strategic themes that align with their style. Mikhail Tal exemplified tactical brilliance with his sacrificial attacks, while Anatoly Karpov demonstrated the power of refined positional technique. Most strong players can play both styles but show preferences and strengths in certain types of positions.

For chess engines, playing style has traditionally been less pronounced. Classical engines tend toward tactical play due to their search depth, while neural network engines often display more positional understanding. Our work explores whether distinct playing styles can be deliberately cultivated and maintained in federated learning settings, creating specialized engines rather than homogeneous ones.

\section{Related Work}

Our work draws on several areas of research: distributed reinforcement learning, federated learning applications to RL, behavioral diversity in multi-agent systems, and clustered federated learning.

\subsection{Distributed Reinforcement Learning}

Distributed training has become essential for reinforcement learning in complex domains due to the computational demands of both environment interaction and neural network training.

\textbf{Parallel Experience Collection:} Many RL systems use multiple actors to collect experience in parallel, dramatically increasing sample efficiency. A3C (Asynchronous Advantage Actor-Critic) introduced asynchronous updates from multiple workers to a shared model. IMPALA (Importance Weighted Actor-Learner Architecture) separates experience collection from learning, using importance sampling to handle the resulting off-policy data.

\textbf{Distributed AlphaZero:} The original AlphaZero training used distributed self-play, with many workers generating games in parallel while a central learner updates the neural network. This architecture enables the massive scale of training required for superhuman performanceâ€”AlphaZero played nearly 5 million games during training.

However, these distributed RL approaches still rely on centralized aggregation and aim for a single global model. They distribute computation for efficiency but do not address the challenge of maintaining behavioral diversity or training multiple specialized models collaboratively.

\subsection{Federated Reinforcement Learning}

Applying federated learning to reinforcement learning is an emerging research area. While traditional supervised federated learning deals with fixed datasets, federated RL must handle the added complexity of exploration, temporal dependencies, and non-stationary data distributions as policies improve.

\textbf{Policy-Based FedRL:} Some approaches extend FedAvg directly to policy gradient methods, aggregating policy network parameters across agents. However, this faces challenges when agents experience different environments or have different reward functions, as policies optimized for different MDPs may not meaningfully average.

\textbf{Value-Based FedRL:} Other work focuses on sharing value function estimates or Q-functions across agents. This can be effective when agents share the same environment but experience different parts of the state space.

\textbf{Exploration vs. Exploitation Trade-offs:} Federated RL introduces unique challenges for exploration. If all agents follow similar exploration strategies, they may collectively fail to explore the state space adequately. Some work addresses this through coordinated exploration strategies or by encouraging diversity in local training.

Most federated RL research has focused on settings where agents face different but related tasks, aiming to share knowledge across task distributions. Our work differs by considering agents working on the same task (chess) but seeking to maintain distinct behavioral strategies rather than converging to a single solution.

\subsection{Behavioral Diversity in Multi-Agent Systems}

Maintaining diversity in multi-agent systems has been studied in several contexts, motivated by applications in team behavior, robust learning, and ensemble methods.

\textbf{Quality Diversity Algorithms:} MAP-Elites and related algorithms explicitly optimize for both performance and behavioral diversity. They maintain archives of solutions that exhibit different behaviors, even if some are suboptimal, creating a diverse collection of strategies.

\textbf{Diversity-Driven Exploration:} In multi-agent RL, some work uses diversity objectives to encourage agents to explore different parts of the state space or learn different policies. This can improve collective exploration efficiency and robustness.

\textbf{Emergent Communication and Specialization:} Research in multi-agent communication has shown that agents can spontaneously develop specialized roles when working toward common goals, with different agents handling different subtasks.

Our work differs from these approaches in that we seek to maintain diversity not just during training but in the final models, and we do so in a federated setting where agents cannot directly observe each other but must coordinate through aggregation.

\subsection{Clustered Federated Learning}

Clustered federated learning recognizes that in heterogeneous settings, forcing all clients to converge to a single global model may be suboptimal. Instead, clients are grouped into clusters, with each cluster maintaining its own model.

\textbf{Automatic Clustering:} Several methods propose to automatically discover clusters during training. Clients are initially assigned to clusters randomly or based on data characteristics, then cluster membership is refined based on model similarity or gradient alignment. This allows the system to discover natural groupings in the data distribution.

\textbf{Multi-Center Federated Learning:} Some approaches maintain multiple global models and allow clients to contribute to the model that best matches their data. This creates a form of competitive federated learning where models specialize to different data distributions.

\textbf{Hierarchical Aggregation:} Similar to our approach, some work uses hierarchical aggregation where updates are first aggregated within clusters, then partial aggregation occurs across clusters. This balances the benefits of local specialization with global knowledge sharing.

However, existing clustered federated learning work focuses on supervised learning tasks with heterogeneous data distributions. The clustering emerges from data heterogeneity rather than being designed to preserve behavioral characteristics. Our work extends these ideas to reinforcement learning where we explicitly initialize and maintain clusters based on strategic playing style, and we introduce selective layer-wise aggregation to balance knowledge transfer with behavioral preservation.

\subsection{Transfer Learning in Deep RL}

Transfer learning in reinforcement learning aims to leverage knowledge from one task to accelerate learning on related tasks. This is relevant to our selective aggregation mechanism.

\textbf{Progressive Neural Networks:} Freeze previously learned networks and add new capacity for new tasks, allowing lateral connections. This prevents catastrophic forgetting but increases model size.

\textbf{Fine-Tuning and Layer Freezing:} Standard practice is to fine-tune pretrained networks on new tasks, often freezing early layers that learn general features while adapting later layers to task specifics.

\textbf{Multi-Task Learning:} Training a single network on multiple related tasks can improve performance on all tasks by learning shared representations. However, this typically assumes tasks are sufficiently similar that a shared representation helps rather than hinders.

Our selective inter-cluster aggregation draws on these insights. We share early feature extraction layers across playing style clusters, analogous to sharing general features in transfer learning, while keeping decision-making layers cluster-specific to preserve strategic differences.

\subsection{Gaps in Existing Work}

While the related work provides valuable insights and techniques, several gaps remain that our research addresses:

\begin{enumerate}
    \item \textbf{Federated RL with Intentional Diversity:} Existing federated RL work focuses on handling unavoidable data heterogeneity, not deliberately maintaining behavioral diversity as a goal.

    \item \textbf{Selective Layer Aggregation:} While some clustered FL work uses hierarchical aggregation, selective layer-wise aggregation based on functional role (feature extraction vs. decision making) is underexplored.

    \item \textbf{Playing Style Preservation:} Chess AI research has not addressed how to maintain distinct playing styles in collaborative training settings where the default would be homogenization.

    \item \textbf{Comprehensive Framework:} No existing work provides an end-to-end system combining clustered federated learning, selective aggregation, and self-play RL for behavioral diversity preservation.
\end{enumerate}

Our work addresses these gaps by developing a framework specifically designed to balance collaborative learning with behavioral preservation in reinforcement learning domains, using chess as a concrete and measurable testbed.
