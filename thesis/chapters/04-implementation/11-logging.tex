\section{Logging and Monitoring}

The logging and monitoring infrastructure provides observability into system behavior throughout training, enabling debugging, performance analysis, and anomaly detection. The implementation uses structured logging for semantic queries, context binding for correlation across distributed components, and performance instrumentation for resource tracking.

\subsection{Structured Logging}

Structured logging is implemented via the \texttt{loguru} library, which provides a modern alternative to Python's standard logging module with automatic exception tracing, lazy evaluation, and native support for structured context. Unlike traditional logging that produces unstructured text strings, structured logging emits log records as dictionaries with well-defined fields, enabling programmatic filtering and analysis.

Log level hierarchy follows standard conventions with five levels in increasing severity: DEBUG (detailed diagnostic information for troubleshooting), INFO (confirmation that operations are proceeding as expected), WARNING (indication of potential issues that do not prevent operation), ERROR (failure of a specific operation that may allow continued execution), and CRITICAL (severe failures requiring immediate attention or system shutdown). The framework configures separate minimum log levels for console output (typically INFO in production, DEBUG during development) and file output (typically DEBUG to capture comprehensive history).

Context binding enables correlation of log messages across distributed federated learning components. The logger supports binding arbitrary key-value pairs to create contextualized logger instances that automatically include bound fields in all log records. For example, at the start of a training round, the server creates a bound logger via \texttt{logger.bind(round=42, cluster="tactical")}, then passes this logger to all functions involved in that round. Subsequent log calls like \texttt{logger.info("Starting aggregation")} automatically include \texttt{round=42} and \texttt{cluster="tactical"} in the log record, enabling filtering of all messages related to a specific round or cluster without manual parameter threading.

Log formatting controls how log records are rendered to console and file outputs. The console formatter uses colorized, human-readable output with the pattern \texttt{"\{time:HH:mm:ss\} | \{level\} | \{name\}:\{function\}:\{line\} - \{message\}"}, where level is color-coded (INFO in cyan, WARNING in yellow, ERROR in red) for visual scanning. The file formatter uses JSON serialization to produce structured log files: each line is a JSON object containing timestamp (ISO 8601 format), level, module, function, line number, message, and all bound context fields. This JSON format enables log ingestion by analysis tools like Elasticsearch, Splunk, or custom scripts without parsing text patterns.

Log rotation prevents unbounded log file growth through automatic file cycling based on size or time. The framework configures rotation at 50 MB file size, creating a new log file when the current file exceeds this threshold. Old files are renamed with sequential suffixes (\texttt{app.log}, \texttt{app.log.1}, \texttt{app.log.2}), and files older than the 10 most recent are deleted. This strategy balances retention of recent history against disk space consumption. For long-running experiments, time-based rotation (e.g., daily at midnight) provides natural segmentation aligned with training epochs.

Exception tracing automatically captures stack traces and variable values when errors occur. When a log message is emitted at ERROR or CRITICAL level within an exception handler, loguru automatically includes the exception type, message, and full traceback in the log record. Additionally, if \texttt{diagnose=True} is configured, loguru captures the values of local variables at each stack frame, enabling root cause analysis without requiring debugger attachment. This feature proved valuable during development for diagnosing serialization errors, WebSocket disconnections, and aggregation failures.

\subsection{Performance Monitoring}

Performance monitoring tracks computational resource consumption and operation latencies to identify bottlenecks, validate scaling assumptions, and ensure efficient resource utilization. The implementation instruments critical code paths with timing decorators and integrates with the SystemMetrics plugin for comprehensive resource tracking.

Training time tracking measures the wall-clock duration of each training round at the node level. Before starting local training, the client records the current time via \texttt{time.perf\_counter()}, which provides high-resolution monotonic time suitable for performance measurement. After training completes, the client computes elapsed time and includes it in the model upload message metadata. The server aggregates training times across nodes to compute mean, median, minimum, and maximum training duration per cluster, logging these statistics for each round. Tracking training time helps identify stragglers (nodes with significantly longer training times that may delay aggregation) and detect performance degradation over time.

Aggregation time measurement quantifies how long the server spends combining model updates. The aggregation module wraps the \texttt{aggregate()} method with a timing context manager that records start time, invokes the aggregation logic, computes elapsed time, and includes it in the returned \texttt{AggregationMetrics}. Separate timing is collected for intra-cluster aggregation (FedAvg within each cluster) and inter-cluster aggregation (selective layer sharing across clusters). Analysis of aggregation times validates that aggregation overhead remains negligible compared to training time (typically $<$ 1\% of round duration) and identifies whether selective aggregation (which involves layer identification and filtering) introduces measurable overhead compared to full aggregation.

Model serialization time tracks the CPU cost of converting PyTorch state dictionaries to transmittable format. The serializer instruments compression and encoding steps separately: compression time (gzip) and encoding time (base64 or raw). Profiling reveals that compression dominates serialization cost (0.5-1.0 seconds) while encoding requires minimal time (0.05-0.1 seconds). Deserialization is similarly instrumented on the client side. Comparing serialization times across training rounds detects whether model size growth (which should not occur for fixed architecture) or degraded compression ratios increase communication overhead.

Memory usage monitoring tracks RAM consumption at the process and system level through optional profiling. When \texttt{ENABLE\_PROFILING} environment variable is set, the framework periodically (every 5 seconds) samples memory usage via \texttt{psutil.Process().memory\_info()} and records resident set size (physical RAM) and virtual memory size. These samples are timestamped and correlated with training phases (data loading, forward pass, backward pass, aggregation) to identify memory leaks or unexpectedly high consumption. Profiling revealed that keeping all training data in memory (initial implementation) consumed excessive RAM, motivating the switch to streaming data loading.

Integration with the SystemMetrics plugin provides comprehensive resource tracking beyond timing measurements. This plugin collects CPU utilization percentage, GPU utilization and memory (via \texttt{nvidia-smi} or similar), network bandwidth consumption, and disk I/O rates at configurable intervals (typically every 30 seconds). The plugin correlates these metrics with round numbers and training phases, enabling visualization of resource usage patterns throughout training. For example, plotting GPU utilization over time confirms that GPUs remain highly utilized during training (80-95\%) rather than sitting idle due to data loading bottlenecks or CPU-bound operations.
