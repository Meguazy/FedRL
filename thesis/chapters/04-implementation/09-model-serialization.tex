\section{Model Serialization}

The model serialization system enables efficient transmission of neural network parameters between clients and server over WebSocket connections. The implementation provides framework-agnostic abstractions supporting both PyTorch and TensorFlow models, with configurable compression and encoding options optimized for network efficiency and JSON message compatibility.

\subsection{Serialization Format}

The \texttt{PyTorchSerializer} class in \texttt{common/model\_serialization.py} implements a multi-stage serialization pipeline that transforms PyTorch state dictionaries into compact, transmittable representations. The serialization process follows four sequential stages: extraction, pickling, compression, and encoding.

Extraction begins by obtaining the model's state dictionary via \texttt{model.state\_dict()}, which returns an ordered dictionary mapping parameter names (strings) to parameter tensors. The state dictionary includes all learnable parameters (convolutional weights, batch normalization parameters, fully connected weights) as well as persistent buffers (batch normalization running statistics). For the AlphaZeroNet architecture with 19 residual blocks, the state dictionary contains approximately 11 million parameters organized across roughly 160 distinct tensors.

Pickling serializes the state dictionary to binary format using Python's pickle protocol version 4. The serializer creates an in-memory bytes buffer via \texttt{io.BytesIO()}, then invokes \texttt{pickle.dump(state\_dict, buffer, protocol=4)} to write the serialized representation. Protocol 4 (introduced in Python 3.4) provides efficient serialization of large numeric arrays, storing tensor data in contiguous binary format rather than per-element encoding. The resulting bytes object typically occupies 40-45 MB for a full AlphaZeroNet model.

Compression reduces serialized size through gzip compression at level 6. The serializer applies \texttt{gzip.compress(pickled\_data, compresslevel=6)} to the pickled bytes, achieving typical compression ratios of 5-7× for neural network weights. The high compression effectiveness stems from regularities in trained weights: many parameters cluster near zero due to weight decay, batch normalization parameters are small, and spatial locality in convolutional filters creates compressible patterns. Level 6 compression balances CPU overhead against size reduction, requiring approximately 0.5-1.0 seconds for compression on a typical CPU.

Encoding converts compressed binary data to JSON-compatible format for WebSocket transmission. When \texttt{encoding='base64'} is configured (the default), the serializer applies \texttt{base64.b64encode(compressed\_data).decode('ascii')} to produce an ASCII string suitable for embedding in JSON messages. Base64 encoding increases size by approximately 33\% (4 characters per 3 bytes), but this overhead is acceptable given the preceding compression. The final serialized representation typically occupies 8-10 MB for a full AlphaZeroNet model, enabling transmission over typical network connections in 1-5 seconds depending on bandwidth.

Deserialization reverses the pipeline to reconstruct the state dictionary. The deserializer accepts a base64 string (or raw bytes if binary encoding was used), decodes via \texttt{base64.b64decode()}, decompresses via \texttt{gzip.decompress()}, and unpickles via \texttt{pickle.loads()} to recover the original state dictionary. The client or server then loads the state dictionary into a model instance using \texttt{model.load\_state\_dict(state\_dict)}, updating all parameters to the transmitted values.

Framework abstraction is achieved through the \texttt{ModelSerializer} abstract base class, which defines the \texttt{serialize()} and \texttt{deserialize()} interface that all framework-specific serializers must implement. This abstraction enables the WebSocket protocol layer to remain framework-agnostic: the same message handling code works with PyTorch, TensorFlow, or future frameworks by simply swapping the serializer instance. The \texttt{get\_serializer(framework='pytorch', compression=True)} factory function instantiates the appropriate serializer based on a string parameter, supporting runtime framework selection.

\subsection{Parameter Differencing}

Parameter differencing optimizes bandwidth consumption by transmitting weight updates rather than full model parameters. Instead of sending the complete updated model $\theta_{\text{new}}$, clients compute and transmit the parameter difference $\Delta\theta = \theta_{\text{new}} - \theta_{\text{old}}$, where $\theta_{\text{old}}$ is the model received from the server at the beginning of the round. The server reconstructs the updated model by adding the difference to its stored version: $\theta_{\text{new}} = \theta_{\text{old}} + \Delta\theta$.

Bandwidth savings are most significant in early training rounds when parameter updates are small relative to total parameter magnitude. In a typical training round, clients perform 50-200 gradient descent updates with learning rate $10^{-4}$ to $10^{-3}$, resulting in parameter changes on the order of 1-5\% of total parameter magnitude. When serialized, these small differences compress more effectively than full parameters because most difference values are near zero, creating highly compressible data. Empirical measurements show that serialized differences occupy 20-40\% the size of serialized full parameters in early training, providing 2.5-5× bandwidth reduction.

Implementation occurs in the client communication layer's model upload logic. Before serializing the updated model, the client subtracts the initial model parameter-wise: for each layer name in the state dictionary, the client computes \texttt{delta[layer] = updated\_state[layer] - initial\_state[layer]} using PyTorch tensor subtraction. The resulting delta dictionary (with identical keys but smaller magnitude tensors) is then serialized and transmitted. The client includes a \texttt{delta\_encoding: true} flag in the message metadata to inform the server that reconstruction is required.

Server reconstruction applies the reverse operation upon receiving a delta-encoded model update. The server retrieves the original model sent to this client at round start from a cache keyed by \texttt{(round\_num, node\_id)}, deserializes the received delta, and adds each delta tensor to the corresponding original tensor: \texttt{reconstructed[layer] = original[layer] + delta[layer]}. The reconstructed state dictionary is then used for aggregation as if a full model had been received. The server discards the cached original models after aggregation completes to free memory.

Bandwidth savings diminish as training progresses and models converge. In late training rounds with small learning rates, parameter changes become very small, but they no longer compress significantly better than the full parameters because the changes are distributed across all parameters rather than concentrated in specific layers. Additionally, the base64 encoding overhead (33\%) applies equally to differences and full parameters. Consequently, the framework makes delta encoding optional and configurable per experiment, enabling users to disable it for later training phases where benefits are marginal.
