\section{Summary}

This thesis investigated clustered federated deep reinforcement learning with selective layer aggregation for chess playstyle preservation. The research addressed a fundamental tension in federated learning: how to enable collaborative knowledge transfer across distributed agents while maintaining distinct behavioral strategies essential for strategic diversity.

We developed a framework that combines three key components: clustered federated learning to group agents by playing style, selective layer aggregation to control which neural network components are shared versus specialized, and comprehensive playstyle metrics to measure behavioral diversity. The framework was implemented as a complete distributed system with coordinator-based aggregation, asynchronous training, and multi-metric evaluation.

The experimental campaign evaluated six configurations across two dimensions. Baselines B1 (full sharing) and B2 (no sharing) established performance and diversity extremes. Partial sharing configurations P1 (share early layers, 23\% of parameters), P2 (share middle layers, 32\%), P3 (share late layers plus heads, 50\%), and P4 (share entire backbone, 86\%) explored the design space of selective aggregation strategies. Each configuration trained tactical and positional clusters on games filtered by ECO opening classifications over 350 rounds, with tactical clusters training on games from sharp tactical openings (Sicilian Defence, King's Gambit) and positional clusters training on games from strategic openings (Queen's Gambit, Nimzo-Indian Defence). Evaluation measured both playing strength via ELO ratings and behavioral characteristics via move type analysis and parameter divergence metrics.

The results revealed several critical findings. P2 emerged as the optimal configuration, achieving 1137.5 ELO (98.9\% of the best configuration B2) while maintaining strong behavioral separation (1.51 tactical score difference between clusters). This demonstrates that moderate middle-layer sharing successfully balances performance and diversity. In contrast, P4's extensive sharing produced the worst performance (937.5 ELO) despite maximum behavioral separation (1.95), indicating that sharing 86\% of parameters creates optimization conflicts that cannot be compensated by independent policy and value heads alone.

Policy heads were identified as critical architectural components encoding strategic identity. P3's complete specialization failure (behavioral separation of -0.02) despite sharing only late layers and heads proved that shared policy heads enforce behavioral homogeneity regardless of training distribution differences or backbone architecture. All configurations with independent policy heads (P1, P2, P4) successfully developed distinct playstyles, while those with shared heads (B1, P3) failed completely.

Surprisingly, complete independence (B2) achieved the highest final performance but showed minimal behavioral separation (0.20), comparable to full sharing (B1 at 0.23). This suggests that architectural constraints from selective sharing may be necessary to drive specialization beyond what emerges naturally from data distribution differences alone. However, P2 demonstrated superior average performance across all training rounds (926.1 vs B2's 922.9), indicating that middle-layer sharing improves training stability and sample efficiency even if final convergence falls slightly below fully independent training.

Temporal analysis validated that specialization stabilizes without reconvergence. All configurations with independent heads exhibited plateau behavior after round 200, with policy head divergence stabilizing at 0.21-0.23 and maintaining separation through round 350. This confirms that selective layer aggregation creates persistent behavioral diversity rather than transient training artifacts.

The hypothesis validation demonstrated mixed but informative results. H1 (clustered FL outperforms centralized training) received mixed support, with P2, P1, and P3 exceeding baseline B1 but P4 underperforming, confirming that appropriate layer selection is critical. H2 (selective aggregation enables controlled specialization) was strongly supported, with partial sharing configurations achieving intermediate divergence levels as predicted. H3 (cross-cluster learning enables knowledge transfer) was rejected for final performance but supported for training efficiency. H4 (playstyle emergence) and H5 (distinct strategies) were partially supported, contingent on independent policy heads. H6 (measurable differences) and H7 (stability over training) received strong support across multiple metrics.

These findings challenge conventional federated learning wisdom that more sharing improves performance. In strategic game domains like chess, extensive parameter sharing creates gradient conflicts between different training objectives, with tactical and positional data pushing the shared backbone toward incompatible optima. The optimal strategy involves moderate selective sharing (approximately 30-35\% of parameters) concentrated in middle layers, maintaining independent policy heads for strategic specialization while enabling knowledge transfer through shared feature extraction.
