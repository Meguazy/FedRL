# Performance Optimization Guide: Fast Data Extraction

## Problem

Compressed `.zst` files require sequential decompression from the beginning. As training progresses:
- **Round 1 (offset 0-100)**: ~30-40 seconds
- **Round 2 (offset 1100)**: ~120-150 seconds  
- **Round 10 (offset 10,000)**: ~15+ minutes
- **Round 50+**: Unbearable

## Solutions

### Option 1: Preprocessed Cache (RECOMMENDED â­)

**One-time preprocessing** that makes ALL subsequent training **INSTANT**.

#### Setup (Run Once)

```bash
# Preprocess database into cache files
./scripts/preprocess_database.sh
```

Or manually:

```bash
uv run python -m chess-federated-learning.data.database_preprocessor \
    --input chess-federated-learning/data/databases/lichess_db_standard_rated_2024-01.pgn.zst \
    --output chess-federated-learning/data/cache \
    --min-rating 2000 \
    --playstyle both
```

**Time:** 30-60 minutes (one time only!)

**Result:**
- Creates `chess-federated-learning/data/cache/tactical_samples.pkl`
- Creates `chess-federated-learning/data/cache/positional_samples.pkl`
- Each contains ALL extracted samples in memory-ready format

#### Training (Automatic)

The supervised trainer will **automatically detect and use cache** if it exists:

```python
# In SupervisedTrainer.__init__
if self.cache_dir.exists():
    tactical_cache = self.cache_dir / "tactical_samples.pkl"
    positional_cache = self.cache_dir / "positional_samples.pkl"
    
    if tactical_cache.exists() and positional_cache.exists():
        self.cache_loader = CachedSampleLoader(str(self.cache_dir))
        self.use_cache = True
        log.success(f"Using cached samples - INSTANT ACCESS!")
```

**Performance:**
- âœ… All rounds: **~2-5 seconds** (just loading from pickle)
- âœ… No more sequential decompression
- âœ… True random access
- âœ… Consistent speed regardless of offset

#### Storage Requirements

- **Tactical cache**: ~2-5 GB
- **Positional cache**: ~2-5 GB
- **Total**: ~10 GB for full cache

### Option 2: Smaller Compressed Chunks

Split the database into smaller compressed files:

```bash
# Split by playstyle and rating ranges
mkdir -p chess-federated-learning/data/databases/chunks

# Extract tactical games 0-10k
zstd -d lichess_db.pgn.zst | head -n 500000 | \
    gzip > chunks/tactical_0-10k.pgn.gz

# Extract tactical games 10k-20k  
zstd -d lichess_db.pgn.zst | tail -n +500001 | head -n 500000 | \
    gzip > chunks/tactical_10k-20k.pgn.gz
```

Then configure each node to use its own chunk.

**Pros:**
- No preprocessing needed
- Smaller file sizes

**Cons:**
- Still slower than cache (~10-30s per round)
- Complex setup per node

### Option 3: Database (Advanced)

Import PGN into SQLite or PostgreSQL with indexes:

```python
# Pseudo-code
import sqlite3

# One-time import
db = sqlite3.connect('chess_games.db')
db.execute('''
    CREATE TABLE games (
        id INTEGER PRIMARY KEY,
        eco TEXT,
        rating INTEGER,
        playstyle TEXT,
        pgn TEXT,
        INDEX idx_playstyle_rating ON (playstyle, rating)
    )
''')

# Fast queries during training
cursor = db.execute('''
    SELECT pgn FROM games 
    WHERE playstyle = ? AND rating >= ?
    LIMIT ? OFFSET ?
''', (playstyle, 2000, 100, offset))
```

**Pros:**
- True random access
- Can query by multiple criteria
- Scales to huge datasets

**Cons:**
- Complex setup
- Need to maintain database

## Recommended Workflow

### For Development/Testing

Use the cache! One-time setup, instant training forever:

```bash
# 1. Preprocess (run once)
./scripts/preprocess_database.sh

# 2. Train (runs at full speed immediately)
uv run python chess-federated-learning/server/main.py &
uv run python chess-federated-learning/scripts/start_all_nodes.py \
    --config-dir chess-federated-learning/config/nodes
```

### For Production/Long Training

Same as development - the cache handles it perfectly. You can train for thousands of rounds without slowdown.

### Regenerating Cache

If you update your database or filters:

```bash
# Delete old cache
rm -rf chess-federated-learning/data/cache/*.pkl

# Regenerate
./scripts/preprocess_database.sh
```

## Performance Comparison

| Method | Round 1 | Round 10 | Round 50 | Round 100 |
|--------|---------|----------|----------|-----------|
| **Compressed (current)** | 30s | 5min | 25min | 50min+ |
| **With cache** | 3s | 3s | 3s | 3s |
| **Database** | 5s | 5s | 5s | 5s |
| **Chunks** | 15s | 15s | 15s | 15s |

## Implementation Details

### Cache Structure

```python
# tactical_samples.pkl
[
    TrainingSample(
        board=chess.Board(...),
        move_played=chess.Move(...),
        game_outcome=1.0,
        move_number=15,
        eco_code="B20",
        playstyle="tactical",
        history=[board_t-1, board_t-2, ...]
    ),
    ...  # ~4-5 million samples
]
```

### Loading Performance

```python
# Load entire cache into memory (first time per playstyle)
with open('tactical_samples.pkl', 'rb') as f:
    samples = pickle.load(f)  # ~5 seconds

# Subsequent access (instant)
batch = samples[offset:offset+num_samples]  # < 1ms
```

### Memory Usage

- **Per node**: Loads only its playstyle cache (~3-5 GB)
- **8 nodes total**: ~24-40 GB RAM across all nodes
- **With shared storage**: Only 1 copy, NFS-mount to all nodes

## Troubleshooting

### Cache not detected

```bash
# Check if cache exists
ls -lh chess-federated-learning/data/cache/

# Should show:
# tactical_samples.pkl
# tactical_metadata.pkl
# positional_samples.pkl  
# positional_metadata.pkl
```

### Preprocessing fails

```python
# Check database path
ls -lh chess-federated-learning/data/databases/*.pgn.zst

# Check free disk space (need ~10GB)
df -h chess-federated-learning/data/cache/
```

### Out of memory during preprocessing

```python
# Reduce chunk size in database_preprocessor.py
chunk_size = 5000  # Down from 10000
```

### Cache corrupted

```bash
# Delete and regenerate
rm chess-federated-learning/data/cache/*.pkl
./scripts/preprocess_database.sh
```

## Next Steps

1. **Run preprocessing**: `./scripts/preprocess_database.sh`
2. **Verify cache**: `ls -lh chess-federated-learning/data/cache/`
3. **Start training**: Nodes will automatically use cache
4. **Enjoy instant extraction**: All rounds now take 2-5 seconds! ðŸš€
